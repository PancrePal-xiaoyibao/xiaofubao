#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡æ¡£è´¨é‡æ‰«æå™¨ - æ£€æŸ¥è½¬æ¢åçš„æ–‡æ¡£åˆ†å—è´¨é‡å’Œç»“æ„
ç”¨äºåˆ†æpreprocess_enhanced.pyå¤„ç†åçš„æ–‡æ¡£ï¼Œè¯„ä¼°åˆ†å—æ•ˆæœå’Œè´¨é‡

ä¸»è¦åŠŸèƒ½ï¼š
- åˆ†æåˆ†å—å¤§å°åˆ†å¸ƒ
- æ£€æŸ¥åˆ†å—è¾¹ç•Œè´¨é‡
- è¯†åˆ«æ½œåœ¨çš„åˆ†å—é—®é¢˜
- ç”Ÿæˆè¯¦ç»†çš„è´¨é‡æŠ¥å‘Š
- éªŒè¯ç« èŠ‚ç»“æ„å®Œæ•´æ€§

ä½œè€…: RAGé¢„å¤„ç†ä¸“å®¶
ç‰ˆæœ¬: 1.0
æ›´æ–°æ—¥æœŸ: 2024
"""

import argparse
import re
import json
from typing import List, Dict, Tuple, Optional
from collections import defaultdict, Counter
import statistics

class DocumentQualityScanner:
    """æ–‡æ¡£è´¨é‡æ‰«æå™¨"""
    
    def __init__(self):
        self.chunks = []
        self.chunk_boundaries = []
        self.statistics = {}
        self.issues = []
        
    def load_document(self, file_path: str) -> bool:
        """
        åŠ è½½å¤„ç†åçš„æ–‡æ¡£
        
        Args:
            file_path (str): æ–‡æ¡£è·¯å¾„
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸåŠ è½½
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æŒ‰[CHUNK_BOUNDARY]åˆ†å‰²æ–‡æ¡£
            chunks = content.split('[CHUNK_BOUNDARY]')
            self.chunks = [chunk.strip() for chunk in chunks if chunk.strip()]
            
            print(f"æˆåŠŸåŠ è½½æ–‡æ¡£: {file_path}")
            print(f"æ£€æµ‹åˆ° {len(self.chunks)} ä¸ªåˆ†å—")
            return True
            
        except Exception as e:
            print(f"åŠ è½½æ–‡æ¡£å¤±è´¥: {e}")
            return False
    
    def analyze_chunk_sizes(self) -> Dict:
        """
        åˆ†æåˆ†å—å¤§å°åˆ†å¸ƒ
        
        Returns:
            Dict: å¤§å°åˆ†æç»“æœ
        """
        sizes = [len(chunk) for chunk in self.chunks]
        
        if not sizes:
            return {}
        
        analysis = {
            'total_chunks': len(sizes),
            'total_characters': sum(sizes),
            'average_size': statistics.mean(sizes),
            'median_size': statistics.median(sizes),
            'min_size': min(sizes),
            'max_size': max(sizes),
            'std_deviation': statistics.stdev(sizes) if len(sizes) > 1 else 0
        }
        
        # å¤§å°åˆ†å¸ƒç»Ÿè®¡
        size_ranges = {
            'very_small': len([s for s in sizes if s < 200]),
            'small': len([s for s in sizes if 200 <= s < 500]),
            'medium': len([s for s in sizes if 500 <= s < 1000]),
            'large': len([s for s in sizes if 1000 <= s < 2000]),
            'very_large': len([s for s in sizes if s >= 2000])
        }
        
        analysis['size_distribution'] = size_ranges
        
        return analysis
    
    def analyze_chunk_boundaries(self) -> Dict:
        """
        åˆ†æåˆ†å—è¾¹ç•Œè´¨é‡
        
        Returns:
            Dict: è¾¹ç•Œåˆ†æç»“æœ
        """
        boundary_types = {
            'markdown_heading': 0,
            'chinese_major_section': 0,
            'chinese_sub_section': 0,
            'numeric_section': 0,
            'table_title': 0,
            'empty_line': 0,
            'other': 0
        }
        
        boundary_quality_scores = []
        
        for i, chunk in enumerate(self.chunks):
            if not chunk:
                continue
                
            lines = chunk.split('\n')
            first_line = lines[0].strip() if lines else ""
            
            # åˆ†æè¾¹ç•Œç±»å‹
            boundary_type = self._classify_boundary(first_line)
            boundary_types[boundary_type] += 1
            
            # è®¡ç®—è¾¹ç•Œè´¨é‡åˆ†æ•°
            quality_score = self._calculate_boundary_quality(first_line, chunk)
            boundary_quality_scores.append(quality_score)
        
        return {
            'boundary_types': boundary_types,
            'average_quality_score': statistics.mean(boundary_quality_scores) if boundary_quality_scores else 0,
            'quality_scores': boundary_quality_scores
        }
    
    def _classify_boundary(self, line: str) -> str:
        """
        åˆ†ç±»è¾¹ç•Œç±»å‹
        
        Args:
            line (str): è¾¹ç•Œè¡Œ
            
        Returns:
            str: è¾¹ç•Œç±»å‹
        """
        if not line:
            return 'empty_line'
        
        # Markdownæ ‡é¢˜
        if line.startswith('#'):
            return 'markdown_heading'
        
        # ä¸­æ–‡ä¸€çº§åºå·
        if re.match(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€', line):
            return 'chinese_major_section'
        
        # ä¸­æ–‡å­åºå·
        if re.match(r'^\([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\)', line):
            return 'chinese_sub_section'
        
        # æ•°å­—æ ‡é¢˜
        if re.match(r'^(<\d+>)?\d{1,2}[\u4e00-\u9fa5]', line):
            return 'numeric_section'
        
        # è¡¨æ ¼æ ‡é¢˜ï¼ˆå†’å·ç»“å°¾ï¼‰
        if line.endswith('ï¼š') or line.endswith(':'):
            return 'table_title'
        
        return 'other'
    
    def _calculate_boundary_quality(self, boundary_line: str, chunk_content: str) -> float:
        """
        è®¡ç®—è¾¹ç•Œè´¨é‡åˆ†æ•°
        
        Args:
            boundary_line (str): è¾¹ç•Œè¡Œ
            chunk_content (str): åˆ†å—å†…å®¹
            
        Returns:
            float: è´¨é‡åˆ†æ•° (0-1)
        """
        score = 0.0
        
        # åŸºç¡€åˆ†æ•°ï¼šæœ‰æ˜ç¡®çš„æ ‡é¢˜ç»“æ„
        if self._classify_boundary(boundary_line) in ['markdown_heading', 'chinese_major_section', 'numeric_section']:
            score += 0.4
        
        # å†…å®¹å®Œæ•´æ€§ï¼šæ£€æŸ¥æ˜¯å¦æœ‰å®Œæ•´çš„æ®µè½
        lines = chunk_content.split('\n')
        non_empty_lines = [line for line in lines if line.strip()]
        
        if len(non_empty_lines) >= 3:  # è‡³å°‘3è¡Œéç©ºå†…å®¹
            score += 0.2
        
        # è¯­ä¹‰è¿è´¯æ€§ï¼šæ£€æŸ¥æ˜¯å¦ä»¥å¥å·ç»“å°¾
        if chunk_content.strip().endswith(('ã€‚', '.', 'ï¼', '!', 'ï¼Ÿ', '?')):
            score += 0.2
        
        # ç»“æ„å®Œæ•´æ€§ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«è¡¨æ ¼æˆ–åˆ—è¡¨
        if '<table>' in chunk_content or re.search(r'^\d+\.', chunk_content, re.MULTILINE):
            score += 0.1
        
        # é•¿åº¦åˆç†æ€§ï¼šé¿å…è¿‡çŸ­æˆ–è¿‡é•¿
        chunk_length = len(chunk_content)
        if 300 <= chunk_length <= 1500:
            score += 0.1
        
        return min(score, 1.0)
    
    def detect_issues(self) -> List[Dict]:
        """
        æ£€æµ‹æ½œåœ¨é—®é¢˜
        
        Returns:
            List[Dict]: é—®é¢˜åˆ—è¡¨
        """
        issues = []
        
        for i, chunk in enumerate(self.chunks):
            chunk_length = len(chunk)
            
            # æ£€æµ‹è¿‡å°çš„åˆ†å—
            if chunk_length < 100:
                issues.append({
                    'type': 'too_small',
                    'chunk_index': i,
                    'description': f'åˆ†å— {i+1} è¿‡å° ({chunk_length} å­—ç¬¦)',
                    'severity': 'medium',
                    'chunk_preview': chunk[:100] + '...' if len(chunk) > 100 else chunk
                })
            
            # æ£€æµ‹è¿‡å¤§çš„åˆ†å—
            if chunk_length > 3000:
                issues.append({
                    'type': 'too_large',
                    'chunk_index': i,
                    'description': f'åˆ†å— {i+1} è¿‡å¤§ ({chunk_length} å­—ç¬¦)',
                    'severity': 'high',
                    'chunk_preview': chunk[:100] + '...'
                })
            
            # æ£€æµ‹ç©ºåˆ†å—
            if not chunk.strip():
                issues.append({
                    'type': 'empty_chunk',
                    'chunk_index': i,
                    'description': f'åˆ†å— {i+1} ä¸ºç©º',
                    'severity': 'high',
                    'chunk_preview': ''
                })
            
            # æ£€æµ‹å¯èƒ½çš„åˆ†å—è¾¹ç•Œé—®é¢˜
            lines = chunk.split('\n')
            first_line = lines[0].strip() if lines else ""
            
            if first_line and not self._is_good_boundary(first_line):
                issues.append({
                    'type': 'poor_boundary',
                    'chunk_index': i,
                    'description': f'åˆ†å— {i+1} è¾¹ç•Œè´¨é‡è¾ƒå·®',
                    'severity': 'low',
                    'chunk_preview': first_line
                })
        
        return issues
    
    def _is_good_boundary(self, line: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºè‰¯å¥½çš„åˆ†å—è¾¹ç•Œ
        
        Args:
            line (str): è¾¹ç•Œè¡Œ
            
        Returns:
            bool: æ˜¯å¦ä¸ºè‰¯å¥½è¾¹ç•Œ
        """
        boundary_type = self._classify_boundary(line)
        return boundary_type in ['markdown_heading', 'chinese_major_section', 'chinese_sub_section', 'numeric_section']
    
    def analyze_content_structure(self) -> Dict:
        """
        åˆ†æå†…å®¹ç»“æ„
        
        Returns:
            Dict: ç»“æ„åˆ†æç»“æœ
        """
        structure_stats = {
            'chunks_with_tables': 0,
            'chunks_with_lists': 0,
            'chunks_with_headings': 0,
            'chunks_with_references': 0,
            'average_lines_per_chunk': 0,
            'content_types': defaultdict(int)
        }
        
        total_lines = 0
        
        for chunk in self.chunks:
            lines = chunk.split('\n')
            total_lines += len(lines)
            
            # æ£€æµ‹è¡¨æ ¼
            if '<table>' in chunk:
                structure_stats['chunks_with_tables'] += 1
                structure_stats['content_types']['table'] += 1
            
            # æ£€æµ‹åˆ—è¡¨
            if re.search(r'^\d+\.', chunk, re.MULTILINE) or re.search(r'^[â€¢\-\*]', chunk, re.MULTILINE):
                structure_stats['chunks_with_lists'] += 1
                structure_stats['content_types']['list'] += 1
            
            # æ£€æµ‹æ ‡é¢˜
            if re.search(r'^#+', chunk, re.MULTILINE):
                structure_stats['chunks_with_headings'] += 1
                structure_stats['content_types']['heading'] += 1
            
            # æ£€æµ‹å¼•ç”¨æˆ–æ³¨é‡Š
            if 'ã€æ³¨é‡Šã€‘' in chunk or 'æ³¨ï¼š' in chunk or re.search(r'\[\d+\]', chunk):
                structure_stats['chunks_with_references'] += 1
                structure_stats['content_types']['reference'] += 1
            
            # æ£€æµ‹çº¯æ–‡æœ¬
            if not any([
                '<table>' in chunk,
                re.search(r'^\d+\.', chunk, re.MULTILINE),
                re.search(r'^#+', chunk, re.MULTILINE),
                'ã€æ³¨é‡Šã€‘' in chunk
            ]):
                structure_stats['content_types']['text'] += 1
        
        structure_stats['average_lines_per_chunk'] = total_lines / len(self.chunks) if self.chunks else 0
        
        return structure_stats
    
    def generate_report(self) -> Dict:
        """
        ç”Ÿæˆå®Œæ•´çš„è´¨é‡æŠ¥å‘Š
        
        Returns:
            Dict: è´¨é‡æŠ¥å‘Š
        """
        print("æ­£åœ¨åˆ†ææ–‡æ¡£è´¨é‡...")
        
        # æ‰§è¡Œå„é¡¹åˆ†æ
        size_analysis = self.analyze_chunk_sizes()
        boundary_analysis = self.analyze_chunk_boundaries()
        structure_analysis = self.analyze_content_structure()
        issues = self.detect_issues()
        
        # è®¡ç®—æ€»ä½“è´¨é‡åˆ†æ•°
        overall_score = self._calculate_overall_score(size_analysis, boundary_analysis, issues)
        
        report = {
            'summary': {
                'total_chunks': len(self.chunks),
                'overall_quality_score': overall_score,
                'total_issues': len(issues),
                'high_severity_issues': len([i for i in issues if i['severity'] == 'high'])
            },
            'size_analysis': size_analysis,
            'boundary_analysis': boundary_analysis,
            'structure_analysis': structure_analysis,
            'issues': issues,
            'recommendations': self._generate_recommendations(issues, size_analysis)
        }
        
        return report
    
    def _calculate_overall_score(self, size_analysis: Dict, boundary_analysis: Dict, issues: List) -> float:
        """
        è®¡ç®—æ€»ä½“è´¨é‡åˆ†æ•°
        
        Args:
            size_analysis (Dict): å¤§å°åˆ†æç»“æœ
            boundary_analysis (Dict): è¾¹ç•Œåˆ†æç»“æœ
            issues (List): é—®é¢˜åˆ—è¡¨
            
        Returns:
            float: æ€»ä½“è´¨é‡åˆ†æ•° (0-1)
        """
        score = 0.0
        
        # å¤§å°åˆ†å¸ƒåˆ†æ•° (30%)
        if size_analysis:
            avg_size = size_analysis.get('average_size', 0)
            if 400 <= avg_size <= 1200:  # ç†æƒ³èŒƒå›´
                score += 0.3
            elif 200 <= avg_size <= 2000:  # å¯æ¥å—èŒƒå›´
                score += 0.2
            else:
                score += 0.1
        
        # è¾¹ç•Œè´¨é‡åˆ†æ•° (40%)
        boundary_score = boundary_analysis.get('average_quality_score', 0)
        score += boundary_score * 0.4
        
        # é—®é¢˜æƒ©ç½š (30%)
        issue_penalty = 0
        for issue in issues:
            if issue['severity'] == 'high':
                issue_penalty += 0.1
            elif issue['severity'] == 'medium':
                issue_penalty += 0.05
            else:
                issue_penalty += 0.02
        
        issue_score = max(0, 0.3 - issue_penalty)
        score += issue_score
        
        return min(score, 1.0)
    
    def _generate_recommendations(self, issues: List, size_analysis: Dict) -> List[str]:
        """
        ç”Ÿæˆæ”¹è¿›å»ºè®®
        
        Args:
            issues (List): é—®é¢˜åˆ—è¡¨
            size_analysis (Dict): å¤§å°åˆ†æç»“æœ
            
        Returns:
            List[str]: å»ºè®®åˆ—è¡¨
        """
        recommendations = []
        
        # åŸºäºé—®é¢˜çš„å»ºè®®
        issue_types = Counter([issue['type'] for issue in issues])
        
        if issue_types.get('too_small', 0) > 5:
            recommendations.append("å»ºè®®è°ƒæ•´åˆ†å—ç­–ç•¥ï¼Œå‡å°‘è¿‡å°çš„åˆ†å—æ•°é‡ï¼Œå¯ä»¥è€ƒè™‘åˆå¹¶ç›¸é‚»çš„å°åˆ†å—")
        
        if issue_types.get('too_large', 0) > 0:
            recommendations.append("å»ºè®®å¯¹è¿‡å¤§çš„åˆ†å—è¿›è¡Œè¿›ä¸€æ­¥åˆ†å‰²ï¼Œä¿æŒåˆ†å—å¤§å°çš„åˆç†æ€§")
        
        if issue_types.get('poor_boundary', 0) > 10:
            recommendations.append("å»ºè®®ä¼˜åŒ–åˆ†å—è¾¹ç•Œæ£€æµ‹é€»è¾‘ï¼Œæé«˜è¾¹ç•Œè¯†åˆ«çš„å‡†ç¡®æ€§")
        
        # åŸºäºå¤§å°åˆ†æçš„å»ºè®®
        if size_analysis:
            avg_size = size_analysis.get('average_size', 0)
            if avg_size < 300:
                recommendations.append("å¹³å‡åˆ†å—å¤§å°åå°ï¼Œå»ºè®®å¢åŠ åˆ†å—çš„æœ€å°å­—ç¬¦æ•°é™åˆ¶")
            elif avg_size > 1500:
                recommendations.append("å¹³å‡åˆ†å—å¤§å°åå¤§ï¼Œå»ºè®®é™ä½åˆ†å—çš„æœ€å¤§å­—ç¬¦æ•°é™åˆ¶")
        
        if not recommendations:
            recommendations.append("æ–‡æ¡£åˆ†å—è´¨é‡è‰¯å¥½ï¼Œæ— éœ€ç‰¹åˆ«è°ƒæ•´")
        
        return recommendations
    
    def print_report(self, report: Dict):
        """
        æ‰“å°æ ¼å¼åŒ–çš„æŠ¥å‘Š
        
        Args:
            report (Dict): è´¨é‡æŠ¥å‘Š
        """
        print("\n" + "="*60)
        print("æ–‡æ¡£è´¨é‡åˆ†ææŠ¥å‘Š")
        print("="*60)
        
        # æ€»ä½“æ¦‚å†µ
        summary = report['summary']
        print(f"\nğŸ“Š æ€»ä½“æ¦‚å†µ:")
        print(f"   æ€»åˆ†å—æ•°: {summary['total_chunks']}")
        print(f"   è´¨é‡åˆ†æ•°: {summary['overall_quality_score']:.2f}/1.00")
        print(f"   å‘ç°é—®é¢˜: {summary['total_issues']} ä¸ª")
        print(f"   ä¸¥é‡é—®é¢˜: {summary['high_severity_issues']} ä¸ª")
        
        # å¤§å°åˆ†æ
        size_analysis = report['size_analysis']
        if size_analysis:
            print(f"\nğŸ“ åˆ†å—å¤§å°åˆ†æ:")
            print(f"   å¹³å‡å¤§å°: {size_analysis['average_size']:.0f} å­—ç¬¦")
            print(f"   ä¸­ä½æ•°: {size_analysis['median_size']:.0f} å­—ç¬¦")
            print(f"   æœ€å°/æœ€å¤§: {size_analysis['min_size']}/{size_analysis['max_size']} å­—ç¬¦")
            print(f"   æ ‡å‡†å·®: {size_analysis['std_deviation']:.0f}")
            
            print(f"\n   å¤§å°åˆ†å¸ƒ:")
            dist = size_analysis['size_distribution']
            print(f"   å¾ˆå°(<200): {dist['very_small']} ä¸ª")
            print(f"   å°(200-500): {dist['small']} ä¸ª")
            print(f"   ä¸­(500-1000): {dist['medium']} ä¸ª")
            print(f"   å¤§(1000-2000): {dist['large']} ä¸ª")
            print(f"   å¾ˆå¤§(>=2000): {dist['very_large']} ä¸ª")
        
        # è¾¹ç•Œåˆ†æ
        boundary_analysis = report['boundary_analysis']
        if boundary_analysis:
            print(f"\nğŸ¯ è¾¹ç•Œè´¨é‡åˆ†æ:")
            print(f"   å¹³å‡è´¨é‡åˆ†æ•°: {boundary_analysis['average_quality_score']:.2f}/1.00")
            
            print(f"\n   è¾¹ç•Œç±»å‹åˆ†å¸ƒ:")
            types = boundary_analysis['boundary_types']
            for boundary_type, count in types.items():
                if count > 0:
                    print(f"   {boundary_type}: {count} ä¸ª")
        
        # ç»“æ„åˆ†æ
        structure_analysis = report['structure_analysis']
        if structure_analysis:
            print(f"\nğŸ—ï¸ å†…å®¹ç»“æ„åˆ†æ:")
            print(f"   åŒ…å«è¡¨æ ¼çš„åˆ†å—: {structure_analysis['chunks_with_tables']} ä¸ª")
            print(f"   åŒ…å«åˆ—è¡¨çš„åˆ†å—: {structure_analysis['chunks_with_lists']} ä¸ª")
            print(f"   åŒ…å«æ ‡é¢˜çš„åˆ†å—: {structure_analysis['chunks_with_headings']} ä¸ª")
            print(f"   åŒ…å«å¼•ç”¨çš„åˆ†å—: {structure_analysis['chunks_with_references']} ä¸ª")
            print(f"   å¹³å‡è¡Œæ•°/åˆ†å—: {structure_analysis['average_lines_per_chunk']:.1f}")
        
        # é—®é¢˜åˆ—è¡¨
        issues = report['issues']
        if issues:
            print(f"\nâš ï¸ å‘ç°çš„é—®é¢˜:")
            for i, issue in enumerate(issues[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ªé—®é¢˜
                severity_icon = "ğŸ”´" if issue['severity'] == 'high' else "ğŸŸ¡" if issue['severity'] == 'medium' else "ğŸŸ¢"
                print(f"   {severity_icon} {issue['description']}")
                if issue['chunk_preview']:
                    preview = issue['chunk_preview'][:50] + "..." if len(issue['chunk_preview']) > 50 else issue['chunk_preview']
                    print(f"      é¢„è§ˆ: {preview}")
            
            if len(issues) > 10:
                print(f"   ... è¿˜æœ‰ {len(issues) - 10} ä¸ªé—®é¢˜")
        
        # æ”¹è¿›å»ºè®®
        recommendations = report['recommendations']
        if recommendations:
            print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
            for i, rec in enumerate(recommendations, 1):
                print(f"   {i}. {rec}")
        
        print("\n" + "="*60)

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ–‡æ¡£è´¨é‡æ‰«æå™¨')
    parser.add_argument('input_file', help='è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆå¤„ç†åçš„æ–‡æ¡£ï¼‰')
    parser.add_argument('--output', '-o', help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰')
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†è¾“å‡º')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ‰«æå™¨
    scanner = DocumentQualityScanner()
    
    # åŠ è½½æ–‡æ¡£
    if not scanner.load_document(args.input_file):
        return 1
    
    # ç”ŸæˆæŠ¥å‘Š
    report = scanner.generate_report()
    
    # æ‰“å°æŠ¥å‘Š
    scanner.print_report(report)
    
    # ä¿å­˜æŠ¥å‘Š
    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        print(f"\næŠ¥å‘Šå·²ä¿å­˜è‡³: {args.output}")
    
    return 0

if __name__ == "__main__":
    exit(main())