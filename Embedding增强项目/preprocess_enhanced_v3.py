#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŒ»å­¦æ–‡æ¡£é¢„å¤„ç†è„šæœ¬ - æœ€ç»ˆä¼˜åŒ–ç‰ˆæœ¬ (V3)
ä¸“é—¨é’ˆå¯¹åŒ»å­¦æŒ‡å—æ–‡æ¡£çš„æ™ºèƒ½åˆ†å—å¤„ç†

ä¸»è¦æ”¹è¿›ï¼š
1. è§£å†³V2åˆ†å—è¿‡å¤§é—®é¢˜ï¼Œç›®æ ‡åˆ†å—å¤§å°400-600å­—ç¬¦
2. å¼ºåˆ¶åˆ†å‰²è¶…è¿‡1500å­—ç¬¦çš„åˆ†å—
3. ä¼˜åŒ–æ•°å­—æ ‡é¢˜æ£€æµ‹çš„ç½®ä¿¡åº¦é˜ˆå€¼
4. å¢åŠ æ›´å¤šè¯­ä¹‰è¾¹ç•Œæ£€æµ‹è§„åˆ™
5. æ”¹è¿›å¼•ç”¨æ–‡çŒ®å¤„ç†é€»è¾‘

åŠŸèƒ½ï¼š
1. æ™ºèƒ½è¯†åˆ«åŒ»å­¦æ–‡æ¡£ç»“æ„
2. åŸºäºè¯­ä¹‰çš„åˆ†å—ç­–ç•¥
3. ä¼˜åŒ–çš„è¾¹ç•Œæ£€æµ‹ç®—æ³•
4. åˆ†å—è´¨é‡è¯„ä¼°
5. è¯¦ç»†çš„å¤„ç†ç»Ÿè®¡

ä½œè€…: RAGé¢„å¤„ç†ä¸“å®¶
ç‰ˆæœ¬: 3.0
"""

import re
import json
import argparse
from typing import List, Dict, Tuple, Any
from dataclasses import dataclass

@dataclass
class ChunkInfo:
    """åˆ†å—ä¿¡æ¯ç±»"""
    content: str
    start_line: int
    end_line: int
    boundary_type: str
    quality_score: float
    size: int

class MedicalDocumentProcessor:
    """åŒ»å­¦æ–‡æ¡£å¤„ç†å™¨ - æœ€ç»ˆä¼˜åŒ–ç‰ˆæœ¬"""
    
    def __init__(self, target_chunk_size: int = 500, max_chunk_size: int = 1500):
        """
        åˆå§‹åŒ–å¤„ç†å™¨
        
        Args:
            target_chunk_size (int): ç›®æ ‡åˆ†å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            max_chunk_size (int): æœ€å¤§åˆ†å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
        """
        self.target_chunk_size = target_chunk_size
        self.max_chunk_size = max_chunk_size
        self.min_chunk_size = 200
        
        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        self._compile_patterns()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_lines': 0,
            'boundary_candidates': 0,
            'chunks_created': 0,
            'forced_splits': 0,
            'quality_scores': []
        }
    
    def _compile_patterns(self):
        """ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼"""
        # æ•°å­—æ ‡é¢˜æ¨¡å¼ï¼ˆä¼˜åŒ–ç½®ä¿¡åº¦ï¼‰
        self.number_title_patterns = [
            (re.compile(r'^<(\d+)>(\d+)\.(\d+)\.(\d+)\s+(.+)$'), 0.95),  # ä¸‰çº§æ•°å­—æ ‡é¢˜
            (re.compile(r'^<(\d+)>(\d+)\.(\d+)\s+(.+)$'), 0.90),        # äºŒçº§æ•°å­—æ ‡é¢˜
            (re.compile(r'^<(\d+)>(\d+)\.\s+(.+)$'), 0.85),             # ä¸€çº§æ•°å­—æ ‡é¢˜
            (re.compile(r'^<(\d+)>(\d+)\s+(.+)$'), 0.80),               # çº¯æ•°å­—æ ‡é¢˜
        ]
        
        # ä¸­æ–‡åºå·æ¨¡å¼
        self.chinese_patterns = [
            (re.compile(r'^<(\d+)>([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)[ã€ï¼]\s*(.+)$'), 0.90),
            (re.compile(r'^<(\d+)>ï¼ˆ([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)ï¼‰\s*(.+)$'), 0.85),
        ]
        
        # Markdownæ ‡é¢˜æ¨¡å¼
        self.markdown_patterns = [
            (re.compile(r'^<(\d+)>(#{1,6})\s+(.+)$'), 0.95),
        ]
        
        # ç‰¹æ®Šç»“æ„æ¨¡å¼
        self.special_patterns = [
            (re.compile(r'^<(\d+)>ã€(.+?)ã€‘'), 0.85),                    # ã€æ ‡é¢˜ã€‘
            (re.compile(r'^<(\d+)>\*\*(.+?)\*\*'), 0.80),               # **ç²—ä½“æ ‡é¢˜**
            (re.compile(r'^<(\d+)>è¡¨\s*\d+'), 0.75),                    # è¡¨æ ¼æ ‡é¢˜
            (re.compile(r'^<(\d+)>å›¾\s*\d+'), 0.75),                    # å›¾ç‰‡æ ‡é¢˜
            (re.compile(r'^<(\d+)>é™„å½•'), 0.85),                        # é™„å½•
            (re.compile(r'^<(\d+)>å‚è€ƒæ–‡çŒ®'), 0.90),                     # å‚è€ƒæ–‡çŒ®
        ]
        
        # å¼•ç”¨æ–‡çŒ®æ¨¡å¼
        self.reference_patterns = [
            (re.compile(r'^<(\d+)>\[\d+\]'), 0.70),                     # [1] æ ¼å¼å¼•ç”¨
            (re.compile(r'^<(\d+)>\d+\.\s*[A-Z]'), 0.75),               # 1. Author æ ¼å¼
        ]
        
        # æ®µè½ç»“æŸæ¨¡å¼
        self.paragraph_end_patterns = [
            re.compile(r'[ã€‚ï¼ï¼Ÿ]$'),                                    # ä¸­æ–‡å¥å·ç»“å°¾
            re.compile(r'[.!?]$'),                                      # è‹±æ–‡å¥å·ç»“å°¾
            re.compile(r'ï¼š$'),                                         # å†’å·ç»“å°¾
        ]
    
    def load_document(self, file_path: str) -> List[str]:
        """
        åŠ è½½æ–‡æ¡£
        
        Args:
            file_path (str): æ–‡æ¡£è·¯å¾„
            
        Returns:
            List[str]: æ–‡æ¡£è¡Œåˆ—è¡¨
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            # ç§»é™¤æ¢è¡Œç¬¦å¹¶è¿‡æ»¤ç©ºè¡Œ
            lines = [line.rstrip('\n\r') for line in lines if line.strip()]
            
            self.stats['total_lines'] = len(lines)
            print(f"ğŸ“„ æ–‡æ¡£åŠ è½½æˆåŠŸ: {len(lines)} è¡Œ")
            
            return lines
            
        except Exception as e:
            print(f"âŒ æ–‡æ¡£åŠ è½½å¤±è´¥: {e}")
            raise
    
    def detect_boundary_candidates(self, lines: List[str]) -> List[Tuple[int, str, float]]:
        """
        æ£€æµ‹è¾¹ç•Œå€™é€‰ç‚¹
        
        Args:
            lines (List[str]): æ–‡æ¡£è¡Œåˆ—è¡¨
            
        Returns:
            List[Tuple[int, str, float]]: (è¡Œå·, è¾¹ç•Œç±»å‹, ç½®ä¿¡åº¦) åˆ—è¡¨
        """
        candidates = []
        
        for i, line in enumerate(lines):
            if not line.strip():
                continue
            
            # æ£€æŸ¥å„ç§æ¨¡å¼
            confidence = 0.0
            boundary_type = "unknown"
            
            # æ•°å­—æ ‡é¢˜æ£€æµ‹
            for pattern, conf in self.number_title_patterns:
                if pattern.match(line):
                    confidence = max(confidence, conf)
                    boundary_type = "number_title"
                    break
            
            # ä¸­æ–‡åºå·æ£€æµ‹
            if confidence < 0.8:
                for pattern, conf in self.chinese_patterns:
                    if pattern.match(line):
                        confidence = max(confidence, conf)
                        boundary_type = "chinese_title"
                        break
            
            # Markdownæ ‡é¢˜æ£€æµ‹
            if confidence < 0.8:
                for pattern, conf in self.markdown_patterns:
                    if pattern.match(line):
                        confidence = max(confidence, conf)
                        boundary_type = "markdown_title"
                        break
            
            # ç‰¹æ®Šç»“æ„æ£€æµ‹
            if confidence < 0.8:
                for pattern, conf in self.special_patterns:
                    if pattern.match(line):
                        confidence = max(confidence, conf)
                        boundary_type = "special_structure"
                        break
            
            # å¼•ç”¨æ–‡çŒ®æ£€æµ‹
            if confidence < 0.7:
                for pattern, conf in self.reference_patterns:
                    if pattern.match(line):
                        confidence = max(confidence, conf)
                        boundary_type = "reference"
                        break
            
            # æ®µè½è¾¹ç•Œæ£€æµ‹ï¼ˆè¾ƒä½ä¼˜å…ˆçº§ï¼‰
            if confidence < 0.5 and i > 0:
                prev_line = lines[i-1].strip()
                if any(pattern.search(prev_line) for pattern in self.paragraph_end_patterns):
                    if len(line) > 10:  # é¿å…çŸ­è¡Œ
                        confidence = 0.4
                        boundary_type = "paragraph"
            
            # è®°å½•å€™é€‰ç‚¹
            if confidence > 0.3:  # é™ä½é˜ˆå€¼ä»¥è·å¾—æ›´å¤šå€™é€‰ç‚¹
                candidates.append((i, boundary_type, confidence))
        
        self.stats['boundary_candidates'] = len(candidates)
        print(f"ğŸ” å‘ç° {len(candidates)} ä¸ªè¾¹ç•Œå€™é€‰ç‚¹")
        
        return candidates
    
    def calculate_chunk_quality(self, content: str, boundary_type: str, confidence: float) -> float:
        """
        è®¡ç®—åˆ†å—è´¨é‡åˆ†æ•°
        
        Args:
            content (str): åˆ†å—å†…å®¹
            boundary_type (str): è¾¹ç•Œç±»å‹
            confidence (float): è¾¹ç•Œç½®ä¿¡åº¦
            
        Returns:
            float: è´¨é‡åˆ†æ•° (0-1)
        """
        score = 0.0
        
        # è¾¹ç•Œç±»å‹æƒé‡
        type_weights = {
            "number_title": 0.4,
            "chinese_title": 0.35,
            "markdown_title": 0.4,
            "special_structure": 0.3,
            "reference": 0.25,
            "paragraph": 0.15,
            "forced_split": 0.1
        }
        
        # è¾¹ç•Œè´¨é‡åˆ†æ•°
        score += type_weights.get(boundary_type, 0.1) * confidence
        
        # å¤§å°è´¨é‡åˆ†æ•°
        size = len(content)
        if self.min_chunk_size <= size <= self.target_chunk_size:
            size_score = 0.4
        elif size <= self.max_chunk_size:
            # çº¿æ€§è¡°å‡
            size_score = 0.4 * (1 - (size - self.target_chunk_size) / (self.max_chunk_size - self.target_chunk_size))
        else:
            size_score = 0.1  # è¶…å¤§åˆ†å—
        
        score += size_score
        
        # å†…å®¹å®Œæ•´æ€§åˆ†æ•°
        content_score = 0.0
        if content.strip().endswith(('ã€‚', '.', 'ï¼', '!', 'ï¼Ÿ', '?')):
            content_score += 0.1
        if len(content.strip()) > 50:
            content_score += 0.1
        
        score += content_score
        
        return min(score, 1.0)
    
    def force_split_large_chunk(self, content: str, start_line: int) -> List[ChunkInfo]:
        """
        å¼ºåˆ¶åˆ†å‰²è¶…å¤§åˆ†å—
        
        Args:
            content (str): åˆ†å—å†…å®¹
            start_line (int): èµ·å§‹è¡Œå·
            
        Returns:
            List[ChunkInfo]: åˆ†å‰²åçš„åˆ†å—åˆ—è¡¨
        """
        chunks = []
        lines = content.split('\n')
        current_chunk = []
        current_size = 0
        current_start = start_line
        
        for i, line in enumerate(lines):
            line_size = len(line)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†å‰²
            if current_size + line_size > self.target_chunk_size and current_chunk:
                # åˆ›å»ºå½“å‰åˆ†å—
                chunk_content = '\n'.join(current_chunk)
                quality = self.calculate_chunk_quality(chunk_content, "forced_split", 0.5)
                
                chunks.append(ChunkInfo(
                    content=chunk_content,
                    start_line=current_start,
                    end_line=current_start + len(current_chunk) - 1,
                    boundary_type="forced_split",
                    quality_score=quality,
                    size=len(chunk_content)
                ))
                
                # é‡ç½®
                current_chunk = [line]
                current_size = line_size
                current_start = start_line + i
                self.stats['forced_splits'] += 1
            else:
                current_chunk.append(line)
                current_size += line_size
        
        # å¤„ç†æœ€åä¸€ä¸ªåˆ†å—
        if current_chunk:
            chunk_content = '\n'.join(current_chunk)
            quality = self.calculate_chunk_quality(chunk_content, "forced_split", 0.5)
            
            chunks.append(ChunkInfo(
                content=chunk_content,
                start_line=current_start,
                end_line=current_start + len(current_chunk) - 1,
                boundary_type="forced_split",
                quality_score=quality,
                size=len(chunk_content)
            ))
        
        return chunks
    
    def create_chunks(self, lines: List[str], candidates: List[Tuple[int, str, float]]) -> List[ChunkInfo]:
        """
        åˆ›å»ºåˆ†å—
        
        Args:
            lines (List[str]): æ–‡æ¡£è¡Œåˆ—è¡¨
            candidates (List[Tuple[int, str, float]]): è¾¹ç•Œå€™é€‰ç‚¹
            
        Returns:
            List[ChunkInfo]: åˆ†å—åˆ—è¡¨
        """
        chunks = []
        
        # æ·»åŠ æ–‡æ¡£å¼€å§‹å’Œç»“æŸè¾¹ç•Œ
        all_boundaries = [(0, "document_start", 1.0)] + candidates + [(len(lines), "document_end", 1.0)]
        all_boundaries.sort(key=lambda x: x[0])
        
        # åŠ¨æ€é€‰æ‹©è¾¹ç•Œç‚¹
        selected_boundaries = [all_boundaries[0]]  # æ€»æ˜¯åŒ…å«å¼€å§‹
        current_size = 0
        
        for i in range(1, len(all_boundaries)):
            boundary_line, boundary_type, confidence = all_boundaries[i]
            
            # è®¡ç®—åˆ°å½“å‰è¾¹ç•Œçš„å†…å®¹å¤§å°
            chunk_lines = lines[selected_boundaries[-1][0]:boundary_line]
            chunk_size = sum(len(line) for line in chunk_lines)
            
            # å†³ç­–é€»è¾‘
            should_split = False
            
            # å¼ºåˆ¶åˆ†å‰²æ¡ä»¶
            if chunk_size > self.max_chunk_size:
                should_split = True
            # ç›®æ ‡å¤§å°æ¡ä»¶
            elif chunk_size >= self.target_chunk_size and confidence > 0.6:
                should_split = True
            # é«˜è´¨é‡è¾¹ç•Œæ¡ä»¶
            elif confidence > 0.85:
                should_split = True
            # æœ€åä¸€ä¸ªè¾¹ç•Œ
            elif i == len(all_boundaries) - 1:
                should_split = True
            
            if should_split:
                selected_boundaries.append((boundary_line, boundary_type, confidence))
        
        # åˆ›å»ºåˆ†å—
        for i in range(len(selected_boundaries) - 1):
            start_line = selected_boundaries[i][0]
            end_line = selected_boundaries[i + 1][0]
            boundary_type = selected_boundaries[i + 1][1]
            confidence = selected_boundaries[i + 1][2]
            
            chunk_lines = lines[start_line:end_line]
            chunk_content = '\n'.join(chunk_lines)
            
            if chunk_content.strip():
                # æ£€æŸ¥æ˜¯å¦éœ€è¦å¼ºåˆ¶åˆ†å‰²
                if len(chunk_content) > self.max_chunk_size:
                    # å¼ºåˆ¶åˆ†å‰²
                    sub_chunks = self.force_split_large_chunk(chunk_content, start_line)
                    chunks.extend(sub_chunks)
                else:
                    # æ­£å¸¸åˆ†å—
                    quality = self.calculate_chunk_quality(chunk_content, boundary_type, confidence)
                    
                    chunk_info = ChunkInfo(
                        content=chunk_content,
                        start_line=start_line,
                        end_line=end_line - 1,
                        boundary_type=boundary_type,
                        quality_score=quality,
                        size=len(chunk_content)
                    )
                    
                    chunks.append(chunk_info)
                    self.stats['quality_scores'].append(quality)
        
        self.stats['chunks_created'] = len(chunks)
        print(f"ğŸ“¦ åˆ›å»º {len(chunks)} ä¸ªåˆ†å—")
        
        return chunks
    
    def save_enhanced_document(self, chunks: List[ChunkInfo], output_path: str, include_metadata: bool = True):
        """
        ä¿å­˜å¢å¼ºæ–‡æ¡£
        
        Args:
            chunks (List[ChunkInfo]): åˆ†å—åˆ—è¡¨
            output_path (str): è¾“å‡ºè·¯å¾„
            include_metadata (bool): æ˜¯å¦åŒ…å«å…ƒæ•°æ®ä¿¡æ¯
        """
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                if include_metadata:
                    f.write("====== æ–‡æ¡£å¼€å§‹ ======\n\n")
                
                for i, chunk in enumerate(chunks):
                    if include_metadata:
                        f.write(f"[CHUNK_{i+1:03d}]\n")
                        f.write(f"è¾¹ç•Œç±»å‹: {chunk.boundary_type}\n")
                        f.write(f"è´¨é‡åˆ†æ•°: {chunk.quality_score:.2f}\n")
                        f.write(f"å¤§å°: {chunk.size} å­—ç¬¦\n")
                        f.write(f"è¡ŒèŒƒå›´: {chunk.start_line+1}-{chunk.end_line+1}\n")
                        f.write("-" * 50 + "\n")
                    
                    f.write(chunk.content)
                    
                    if include_metadata:
                        f.write("\n\n[CHUNK_BOUNDARY]\n\n")
                    else:
                        # çº¯å†…å®¹æ¨¡å¼ä¸‹ï¼Œåˆ†å—ä¹‹é—´ç”¨åŒæ¢è¡Œåˆ†éš”
                        if i < len(chunks) - 1:  # ä¸æ˜¯æœ€åä¸€ä¸ªåˆ†å—
                            f.write("\n\n")
                
                if include_metadata:
                    f.write("====== æ–‡æ¡£ç»“æŸ ======\n")
            
            print(f"ğŸ’¾ å¢å¼ºæ–‡æ¡£å·²ä¿å­˜è‡³: {output_path}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜å¢å¼ºæ–‡æ¡£å¤±è´¥: {e}")
            raise
    
    def generate_statistics(self) -> Dict[str, Any]:
        """
        ç”Ÿæˆå¤„ç†ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict[str, Any]: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        if not self.stats['quality_scores']:
            avg_quality = 0
            boundary_quality = 0
        else:
            avg_quality = sum(self.stats['quality_scores']) / len(self.stats['quality_scores'])
            # è¾¹ç•Œè´¨é‡åŸºäºé«˜è´¨é‡åˆ†å—çš„æ¯”ä¾‹
            high_quality_count = sum(1 for score in self.stats['quality_scores'] if score > 0.7)
            boundary_quality = high_quality_count / len(self.stats['quality_scores'])
        
        # åˆ†å—å¤§å°åˆ†å¸ƒ
        chunk_sizes = [chunk.size for chunk in getattr(self, 'chunks', [])]
        size_distribution = {
            'small': sum(1 for size in chunk_sizes if size < 400),
            'medium': sum(1 for size in chunk_sizes if 400 <= size <= 800),
            'large': sum(1 for size in chunk_sizes if 800 < size <= 1500),
            'extra_large': sum(1 for size in chunk_sizes if size > 1500)
        }
        
        return {
            'processing_info': {
                'total_lines_processed': self.stats['total_lines'],
                'boundary_candidates_found': self.stats['boundary_candidates'],
                'chunks_created': self.stats['chunks_created'],
                'forced_splits': self.stats['forced_splits']
            },
            'statistics': {
                'total_chunks': self.stats['chunks_created'],
                'average_chunk_size': sum(chunk_sizes) / len(chunk_sizes) if chunk_sizes else 0,
                'average_quality_score': avg_quality,
                'boundary_quality_score': boundary_quality,
                'size_distribution': size_distribution
            },
            'quality_analysis': {
                'high_quality_chunks': sum(1 for score in self.stats['quality_scores'] if score > 0.7),
                'medium_quality_chunks': sum(1 for score in self.stats['quality_scores'] if 0.5 <= score <= 0.7),
                'low_quality_chunks': sum(1 for score in self.stats['quality_scores'] if score < 0.5),
                'quality_scores': self.stats['quality_scores']
            }
        }
    
    def save_statistics(self, stats: Dict[str, Any], output_path: str):
        """
        ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            stats (Dict[str, Any]): ç»Ÿè®¡ä¿¡æ¯
            output_path (str): è¾“å‡ºè·¯å¾„
        """
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(stats, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜è‡³: {output_path}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            raise
    
    def process_document(self, input_path: str, output_path: str, stats_path: str = None, include_metadata: bool = True):
        """
        å¤„ç†æ–‡æ¡£çš„ä¸»å‡½æ•°
        
        Args:
            input_path (str): è¾“å…¥æ–‡æ¡£è·¯å¾„
            output_path (str): è¾“å‡ºæ–‡æ¡£è·¯å¾„
            stats_path (str): ç»Ÿè®¡ä¿¡æ¯è¾“å‡ºè·¯å¾„ï¼ˆå¯é€‰ï¼‰
            include_metadata (bool): æ˜¯å¦åœ¨è¾“å‡ºæ–‡æ¡£ä¸­åŒ…å«å…ƒæ•°æ®ä¿¡æ¯
        """
        print("ğŸš€ å¼€å§‹å¤„ç†åŒ»å­¦æ–‡æ¡£...")
        print(f"ğŸ“‹ ç›®æ ‡åˆ†å—å¤§å°: {self.target_chunk_size} å­—ç¬¦")
        print(f"ğŸ“‹ æœ€å¤§åˆ†å—å¤§å°: {self.max_chunk_size} å­—ç¬¦")
        print(f"ğŸ“‹ åŒ…å«å…ƒæ•°æ®: {'æ˜¯' if include_metadata else 'å¦'}")
        
        # åŠ è½½æ–‡æ¡£
        lines = self.load_document(input_path)
        
        # æ£€æµ‹è¾¹ç•Œå€™é€‰ç‚¹
        candidates = self.detect_boundary_candidates(lines)
        
        # åˆ›å»ºåˆ†å—
        self.chunks = self.create_chunks(lines, candidates)
        
        # ä¿å­˜å¢å¼ºæ–‡æ¡£
        self.save_enhanced_document(self.chunks, output_path, include_metadata)
        
        # ç”Ÿæˆå¹¶ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats = self.generate_statistics()
        if stats_path:
            self.save_statistics(stats, stats_path)
        
        # æ‰“å°å¤„ç†æ‘˜è¦
        print("\n" + "="*50)
        print("ğŸ“Š å¤„ç†æ‘˜è¦:")
        print(f"  æ€»è¡Œæ•°: {stats['processing_info']['total_lines_processed']}")
        print(f"  è¾¹ç•Œå€™é€‰ç‚¹: {stats['processing_info']['boundary_candidates_found']}")
        print(f"  åˆ›å»ºåˆ†å—æ•°: {stats['processing_info']['chunks_created']}")
        print(f"  å¼ºåˆ¶åˆ†å‰²æ¬¡æ•°: {stats['processing_info']['forced_splits']}")
        print(f"  å¹³å‡åˆ†å—å¤§å°: {stats['statistics']['average_chunk_size']:.0f} å­—ç¬¦")
        print(f"  å¹³å‡è´¨é‡åˆ†æ•°: {stats['statistics']['average_quality_score']:.2f}")
        print(f"  è¾¹ç•Œè´¨é‡åˆ†æ•°: {stats['statistics']['boundary_quality_score']:.2f}")
        print(f"  å¤§å°åˆ†å¸ƒ: å°({stats['statistics']['size_distribution']['small']}) "
              f"ä¸­({stats['statistics']['size_distribution']['medium']}) "
              f"å¤§({stats['statistics']['size_distribution']['large']}) "
              f"è¶…å¤§({stats['statistics']['size_distribution']['extra_large']})")
        print("="*50)
        
        print("âœ… æ–‡æ¡£å¤„ç†å®Œæˆ!")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='åŒ»å­¦æ–‡æ¡£é¢„å¤„ç†è„šæœ¬ - æœ€ç»ˆä¼˜åŒ–ç‰ˆæœ¬')
    parser.add_argument('input_file', help='è¾“å…¥æ–‡æ¡£è·¯å¾„')
    parser.add_argument('--output', '-o', required=True, help='è¾“å‡ºæ–‡æ¡£è·¯å¾„')
    parser.add_argument('--stats-output', help='ç»Ÿè®¡ä¿¡æ¯è¾“å‡ºè·¯å¾„')
    parser.add_argument('--target-size', type=int, default=500, help='ç›®æ ‡åˆ†å—å¤§å°ï¼ˆé»˜è®¤500å­—ç¬¦ï¼‰')
    parser.add_argument('--max-size', type=int, default=1500, help='æœ€å¤§åˆ†å—å¤§å°ï¼ˆé»˜è®¤1500å­—ç¬¦ï¼‰')
    parser.add_argument('--no-metadata', action='store_true', help='ç”Ÿæˆä¸åŒ…å«å…ƒæ•°æ®çš„çº¯å†…å®¹ç‰ˆæœ¬')
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºå¤„ç†å™¨
        processor = MedicalDocumentProcessor(
            target_chunk_size=args.target_size,
            max_chunk_size=args.max_size
        )
        
        # å¤„ç†æ–‡æ¡£
        processor.process_document(
            input_path=args.input_file,
            output_path=args.output,
            stats_path=args.stats_output,
            include_metadata=not args.no_metadata
        )
        
        return 0
        
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        return 1

if __name__ == "__main__":
    exit(main())