#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆæ–‡æ¡£é¢„å¤„ç†å™¨ V2 - åŸºäºè´¨é‡åˆ†æç»“æœçš„ä¼˜åŒ–ç‰ˆæœ¬
ä¸“é—¨é’ˆå¯¹è¾¹ç•Œæ£€æµ‹è´¨é‡å·®çš„é—®é¢˜è¿›è¡Œæ·±åº¦ä¼˜åŒ–

ä¸»è¦æ”¹è¿›ï¼š
1. é‡æ–°è®¾è®¡æ•°å­—æ ‡è®°è¾¹ç•Œæ£€æµ‹ç®—æ³•ï¼Œæå‡è¾¹ç•Œè´¨é‡
2. ä¼˜åŒ–åˆ†å—å¤§å°æ§åˆ¶ï¼Œå‡å°‘è¶…å¤§åˆ†å—
3. æ”¹è¿›å¼•ç”¨æ–‡çŒ®å¤„ç†é€»è¾‘
4. å¢å¼ºè¯­ä¹‰è¾¹ç•Œæ£€æµ‹èƒ½åŠ›
5. æä¾›æ›´ç²¾ç¡®çš„è¾¹ç•Œè´¨é‡è¯„ä¼°

ä½œè€…: RAGé¢„å¤„ç†ä¸“å®¶
ç‰ˆæœ¬: 2.0
åŸºäº: preprocess_enhanced.py + è´¨é‡åˆ†æä¼˜åŒ–
"""

import re
import argparse
import json
from typing import List, Tuple, Dict, Optional
from dataclasses import dataclass
from enum import Enum

class BoundaryType(Enum):
    """è¾¹ç•Œç±»å‹æšä¸¾"""
    DOCUMENT_START = "document_start"
    DOCUMENT_END = "document_end"
    MAJOR_SECTION = "major_section"
    MARKDOWN_HEADING = "markdown_heading"
    NUMERIC_TITLE = "numeric_title"
    TABLE_BOUNDARY = "table_boundary"
    REFERENCE_SECTION = "reference_section"
    PARAGRAPH_BREAK = "paragraph_break"
    SEMANTIC_BREAK = "semantic_break"

@dataclass
class BoundaryCandidate:
    """è¾¹ç•Œå€™é€‰ç‚¹æ•°æ®ç±»"""
    line_index: int
    boundary_type: BoundaryType
    priority: float
    content: str
    context_before: str = ""
    context_after: str = ""
    quality_score: float = 0.0

class EnhancedDocumentProcessor:
    """å¢å¼ºç‰ˆæ–‡æ¡£å¤„ç†å™¨"""
    
    def __init__(self, target_chunk_size: int = 800, max_chunk_size: int = 1500):
        """
        åˆå§‹åŒ–å¤„ç†å™¨
        
        Args:
            target_chunk_size (int): ç›®æ ‡åˆ†å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            max_chunk_size (int): æœ€å¤§åˆ†å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
        """
        self.target_chunk_size = target_chunk_size
        self.max_chunk_size = max_chunk_size
        self.min_chunk_size = 200
        
        # ä¼˜åŒ–åçš„è¾¹ç•Œæ£€æµ‹é…ç½®
        self.boundary_priorities = {
            BoundaryType.DOCUMENT_START: 1.0,
            BoundaryType.DOCUMENT_END: 1.0,
            BoundaryType.MAJOR_SECTION: 0.95,
            BoundaryType.NUMERIC_TITLE: 0.90,  # æå‡æ•°å­—æ ‡é¢˜ä¼˜å…ˆçº§
            BoundaryType.MARKDOWN_HEADING: 0.85,
            BoundaryType.TABLE_BOUNDARY: 0.80,
            BoundaryType.REFERENCE_SECTION: 0.75,
            BoundaryType.SEMANTIC_BREAK: 0.70,
            BoundaryType.PARAGRAPH_BREAK: 0.60
        }
        
        # æ•°å­—æ ‡è®°æ¨¡å¼ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
        self.numeric_patterns = [
            r'^<(\d+)>\s*(.+)$',  # <1007>æ ‡é¢˜å†…å®¹
            r'^(\d+)\.?\s+(.+)$',  # 1. æ ‡é¢˜å†…å®¹ æˆ– 1 æ ‡é¢˜å†…å®¹
            r'^ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+)[ç« èŠ‚éƒ¨åˆ†]\s*(.+)$',  # ç¬¬ä¸€ç«  æ ‡é¢˜å†…å®¹
            r'^[ï¼ˆ(](\d+)[ï¼‰)]\s*(.+)$',  # (1) æ ‡é¢˜å†…å®¹
            r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€ï¼.]\s*(.+)$'  # ä¸€ã€æ ‡é¢˜å†…å®¹
        ]
        
        # è¯­ä¹‰è¾¹ç•Œå…³é”®è¯ï¼ˆæ‰©å±•ç‰ˆï¼‰
        self.semantic_keywords = {
            'section_start': ['æ¦‚è¿°', 'èƒŒæ™¯', 'ç›®çš„', 'æ–¹æ³•', 'ç»“æœ', 'ç»“è®º', 'è®¨è®º', 'å»ºè®®', 'æ€»ç»“'],
            'medical_terms': ['è¯Šæ–­', 'æ²»ç–—', 'é¢„å', 'å¹¶å‘ç—‡', 'å‰¯ä½œç”¨', 'ç¦å¿Œ', 'é€‚åº”ç—‡'],
            'reference_markers': ['å‚è€ƒæ–‡çŒ®', 'å¼•ç”¨', 'æ–‡çŒ®', 'èµ„æ–™æ¥æº'],
            'table_markers': ['è¡¨æ ¼', 'å›¾è¡¨', 'æ•°æ®', 'ç»Ÿè®¡'],
            'conclusion_markers': ['å› æ­¤', 'ç»¼ä¸Š', 'æ€»ä¹‹', 'ç»¼åˆä»¥ä¸Š', 'åŸºäºä»¥ä¸Š']
        }
    
    def load_document(self, file_path: str) -> List[str]:
        """
        åŠ è½½æ–‡æ¡£å¹¶æŒ‰è¡Œåˆ†å‰²
        
        Args:
            file_path (str): æ–‡æ¡£æ–‡ä»¶è·¯å¾„
            
        Returns:
            List[str]: æ–‡æ¡£è¡Œåˆ—è¡¨
            
        Raises:
            Exception: æ–‡ä»¶è¯»å–å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            # æ¸…ç†è¡Œå†…å®¹ï¼Œä¿ç•™åŸå§‹ç»“æ„
            cleaned_lines = []
            for line in lines:
                # ä¿ç•™åŸå§‹æ¢è¡Œç¬¦ï¼Œåªå»é™¤è¡Œå°¾ç©ºç™½
                cleaned_line = line.rstrip()
                cleaned_lines.append(cleaned_line)
            
            print(f"âœ… æˆåŠŸåŠ è½½æ–‡æ¡£: {len(cleaned_lines)} è¡Œ")
            return cleaned_lines
            
        except Exception as e:
            print(f"âŒ æ–‡æ¡£åŠ è½½å¤±è´¥: {e}")
            raise
    
    def detect_numeric_title_enhanced(self, line: str) -> Tuple[bool, float, str]:
        """
        å¢å¼ºç‰ˆæ•°å­—æ ‡é¢˜æ£€æµ‹
        
        Args:
            line (str): å¾…æ£€æµ‹çš„è¡Œ
            
        Returns:
            Tuple[bool, float, str]: (æ˜¯å¦ä¸ºæ•°å­—æ ‡é¢˜, ç½®ä¿¡åº¦, æå–çš„æ ‡é¢˜å†…å®¹)
        """
        line = line.strip()
        if not line:
            return False, 0.0, ""
        
        # æ£€æµ‹å„ç§æ•°å­—æ ‡é¢˜æ¨¡å¼
        for pattern in self.numeric_patterns:
            match = re.match(pattern, line)
            if match:
                # è®¡ç®—ç½®ä¿¡åº¦
                confidence = 0.8
                
                # æ ¹æ®æ¨¡å¼ç±»å‹è°ƒæ•´ç½®ä¿¡åº¦
                if pattern.startswith(r'^<(\d+)>'):  # <1007>æ ¼å¼
                    confidence = 0.95
                elif pattern.startswith(r'^(\d+)\.'):  # 1. æ ¼å¼
                    confidence = 0.90
                elif 'ç« èŠ‚éƒ¨åˆ†' in pattern:  # ç¬¬ä¸€ç« æ ¼å¼
                    confidence = 0.85
                
                # æ ¹æ®å†…å®¹é•¿åº¦è°ƒæ•´ç½®ä¿¡åº¦
                groups = match.groups()
                title_content = groups[-1] if groups else ""
                if len(title_content) > 50:  # æ ‡é¢˜è¿‡é•¿ï¼Œå¯èƒ½ä¸æ˜¯çœŸæ­£çš„æ ‡é¢˜
                    confidence *= 0.8
                elif len(title_content) < 5:  # æ ‡é¢˜è¿‡çŸ­ï¼Œå¯èƒ½ä¸å®Œæ•´
                    confidence *= 0.9
                
                return True, confidence, title_content
        
        return False, 0.0, ""
    
    def detect_semantic_boundary(self, line: str, context_lines: List[str], line_index: int) -> float:
        """
        æ£€æµ‹è¯­ä¹‰è¾¹ç•Œå¼ºåº¦
        
        Args:
            line (str): å½“å‰è¡Œ
            context_lines (List[str]): ä¸Šä¸‹æ–‡è¡Œ
            line_index (int): å½“å‰è¡Œç´¢å¼•
            
        Returns:
            float: è¯­ä¹‰è¾¹ç•Œå¼ºåº¦ (0.0-1.0)
        """
        line = line.strip()
        if not line:
            return 0.0
        
        semantic_score = 0.0
        
        # æ£€æµ‹è¯­ä¹‰å…³é”®è¯
        for category, keywords in self.semantic_keywords.items():
            for keyword in keywords:
                if keyword in line:
                    if category == 'section_start':
                        semantic_score += 0.3
                    elif category == 'medical_terms':
                        semantic_score += 0.2
                    elif category == 'reference_markers':
                        semantic_score += 0.4
                    elif category == 'conclusion_markers':
                        semantic_score += 0.25
        
        # æ£€æµ‹æ®µè½ç»“æ„å˜åŒ–
        if line_index > 0 and line_index < len(context_lines) - 1:
            prev_line = context_lines[line_index - 1].strip()
            next_line = context_lines[line_index + 1].strip()
            
            # å‰ä¸€è¡Œä¸ºç©ºï¼Œå½“å‰è¡Œéç©ºï¼Œä¸‹ä¸€è¡Œéç©º - å¯èƒ½æ˜¯æ–°æ®µè½å¼€å§‹
            if not prev_line and line and next_line:
                semantic_score += 0.2
            
            # å½“å‰è¡Œè¾ƒçŸ­ï¼Œä¸‹ä¸€è¡Œå¼€å§‹æ–°çš„å†…å®¹æ¨¡å¼
            if len(line) < 30 and len(next_line) > 50:
                semantic_score += 0.15
        
        return min(semantic_score, 1.0)
    
    def calculate_boundary_quality(self, candidate: BoundaryCandidate, context_lines: List[str]) -> float:
        """
        è®¡ç®—è¾¹ç•Œè´¨é‡åˆ†æ•°ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
        
        Args:
            candidate (BoundaryCandidate): è¾¹ç•Œå€™é€‰ç‚¹
            context_lines (List[str]): ä¸Šä¸‹æ–‡è¡Œ
            
        Returns:
            float: è´¨é‡åˆ†æ•° (0.0-1.0)
        """
        quality_score = 0.0
        
        # åŸºç¡€ä¼˜å…ˆçº§åˆ†æ•°
        base_priority = self.boundary_priorities.get(candidate.boundary_type, 0.5)
        quality_score += base_priority * 0.4
        
        # å†…å®¹è´¨é‡è¯„ä¼°
        content = candidate.content.strip()
        
        # å†…å®¹é•¿åº¦åˆç†æ€§
        if 5 <= len(content) <= 100:
            quality_score += 0.2
        elif len(content) > 100:
            quality_score += 0.1
        
        # æ•°å­—æ ‡é¢˜ç‰¹æ®Šå¤„ç†
        if candidate.boundary_type == BoundaryType.NUMERIC_TITLE:
            is_numeric, confidence, title = self.detect_numeric_title_enhanced(content)
            if is_numeric:
                quality_score += confidence * 0.3
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºå®Œæ•´çš„æ ‡é¢˜è¡Œ
            if not content.endswith(('ï¼Œ', 'ã€', 'ï¼›')):  # ä¸ä»¥æ ‡ç‚¹ç¬¦å·ç»“å°¾
                quality_score += 0.1
        
        # ä¸Šä¸‹æ–‡ä¸€è‡´æ€§
        if candidate.line_index > 0 and candidate.line_index < len(context_lines) - 1:
            prev_line = context_lines[candidate.line_index - 1].strip()
            next_line = context_lines[candidate.line_index + 1].strip()
            
            # å‰åè¡Œçš„å†…å®¹å·®å¼‚åº¦
            if prev_line and next_line:
                # å¦‚æœå‰ä¸€è¡Œå¾ˆçŸ­æˆ–ä¸ºç©ºï¼Œå½“å‰è¡Œæ˜¯æ ‡é¢˜ï¼Œä¸‹ä¸€è¡Œæ˜¯å†…å®¹
                if len(prev_line) < 20 and len(content) < 50 and len(next_line) > 30:
                    quality_score += 0.15
        
        # è¯­ä¹‰è¾¹ç•Œå¼ºåº¦
        semantic_strength = self.detect_semantic_boundary(content, context_lines, candidate.line_index)
        quality_score += semantic_strength * 0.15
        
        return min(quality_score, 1.0)
    
    def find_boundary_candidates(self, lines: List[str]) -> List[BoundaryCandidate]:
        """
        æŸ¥æ‰¾æ‰€æœ‰è¾¹ç•Œå€™é€‰ç‚¹ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
        
        Args:
            lines (List[str]): æ–‡æ¡£è¡Œåˆ—è¡¨
            
        Returns:
            List[BoundaryCandidate]: è¾¹ç•Œå€™é€‰ç‚¹åˆ—è¡¨
        """
        candidates = []
        
        for i, line in enumerate(lines):
            line_content = line.strip()
            
            # è·³è¿‡ç©ºè¡Œå’Œè¿‡çŸ­çš„è¡Œ
            if not line_content or len(line_content) < 3:
                continue
            
            # æ–‡æ¡£å¼€å§‹å’Œç»“æŸ
            if i == 0:
                candidates.append(BoundaryCandidate(
                    line_index=i,
                    boundary_type=BoundaryType.DOCUMENT_START,
                    priority=1.0,
                    content=line_content
                ))
                continue
            elif i == len(lines) - 1:
                candidates.append(BoundaryCandidate(
                    line_index=i,
                    boundary_type=BoundaryType.DOCUMENT_END,
                    priority=1.0,
                    content=line_content
                ))
                continue
            
            # ä¸»è¦åˆ†éš”ç¬¦
            if '=====' in line_content:
                candidates.append(BoundaryCandidate(
                    line_index=i,
                    boundary_type=BoundaryType.MAJOR_SECTION,
                    priority=0.95,
                    content=line_content
                ))
                continue
            
            # æ•°å­—æ ‡é¢˜æ£€æµ‹ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
            is_numeric, confidence, title_content = self.detect_numeric_title_enhanced(line_content)
            if is_numeric and confidence > 0.7:
                candidates.append(BoundaryCandidate(
                    line_index=i,
                    boundary_type=BoundaryType.NUMERIC_TITLE,
                    priority=confidence * 0.9,  # æ ¹æ®ç½®ä¿¡åº¦è°ƒæ•´ä¼˜å…ˆçº§
                    content=line_content
                ))
                continue
            
            # Markdownæ ‡é¢˜
            if line_content.startswith('#'):
                level = len(line_content) - len(line_content.lstrip('#'))
                priority = max(0.85 - (level - 1) * 0.1, 0.6)  # æ ¹æ®æ ‡é¢˜çº§åˆ«è°ƒæ•´ä¼˜å…ˆçº§
                candidates.append(BoundaryCandidate(
                    line_index=i,
                    boundary_type=BoundaryType.MARKDOWN_HEADING,
                    priority=priority,
                    content=line_content
                ))
                continue
            
            # è¡¨æ ¼è¾¹ç•Œ
            if line_content.startswith('|') and line_content.endswith('|'):
                # æ£€æŸ¥æ˜¯å¦ä¸ºè¡¨æ ¼åˆ†éš”è¡Œ
                if re.match(r'^\|[\s\-\|:]+\|$', line_content):
                    candidates.append(BoundaryCandidate(
                        line_index=i,
                        boundary_type=BoundaryType.TABLE_BOUNDARY,
                        priority=0.80,
                        content=line_content
                    ))
                    continue
            
            # å¼•ç”¨æ–‡çŒ®
            if re.match(r'^\[\d+\]', line_content):
                candidates.append(BoundaryCandidate(
                    line_index=i,
                    boundary_type=BoundaryType.REFERENCE_SECTION,
                    priority=0.75,
                    content=line_content
                ))
                continue
            
            # è¯­ä¹‰è¾¹ç•Œ
            semantic_strength = self.detect_semantic_boundary(line_content, lines, i)
            if semantic_strength > 0.5:
                candidates.append(BoundaryCandidate(
                    line_index=i,
                    boundary_type=BoundaryType.SEMANTIC_BREAK,
                    priority=semantic_strength * 0.7,
                    content=line_content
                ))
        
        # è®¡ç®—æ¯ä¸ªå€™é€‰ç‚¹çš„è´¨é‡åˆ†æ•°
        for candidate in candidates:
            candidate.quality_score = self.calculate_boundary_quality(candidate, lines)
        
        # æŒ‰è´¨é‡åˆ†æ•°æ’åº
        candidates.sort(key=lambda x: (x.quality_score, x.priority), reverse=True)
        
        print(f"ğŸ” å‘ç° {len(candidates)} ä¸ªè¾¹ç•Œå€™é€‰ç‚¹")
        return candidates
    
    def select_optimal_boundaries(self, candidates: List[BoundaryCandidate], lines: List[str]) -> List[int]:
        """
        é€‰æ‹©æœ€ä¼˜è¾¹ç•Œç‚¹ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
        
        Args:
            candidates (List[BoundaryCandidate]): è¾¹ç•Œå€™é€‰ç‚¹åˆ—è¡¨
            lines (List[str]): æ–‡æ¡£è¡Œåˆ—è¡¨
            
        Returns:
            List[int]: é€‰ä¸­çš„è¾¹ç•Œè¡Œç´¢å¼•åˆ—è¡¨
        """
        if not candidates:
            return []
        
        selected_boundaries = []
        total_chars = sum(len(line) for line in lines)
        
        # å¼ºåˆ¶æ·»åŠ æ–‡æ¡£å¼€å§‹
        doc_start = next((c for c in candidates if c.boundary_type == BoundaryType.DOCUMENT_START), None)
        if doc_start:
            selected_boundaries.append(doc_start.line_index)
        
        # æŒ‰è´¨é‡åˆ†æ•°å’Œä¼˜å…ˆçº§é€‰æ‹©è¾¹ç•Œ
        current_pos = 0
        current_chunk_size = 0
        
        for candidate in candidates:
            if candidate.line_index <= current_pos:
                continue
            
            # è®¡ç®—åˆ°å½“å‰å€™é€‰ç‚¹çš„å­—ç¬¦æ•°
            chunk_size = sum(len(lines[i]) for i in range(current_pos, candidate.line_index + 1))
            
            # å†³ç­–é€»è¾‘ä¼˜åŒ–
            should_select = False
            
            # é«˜è´¨é‡è¾¹ç•Œä¼˜å…ˆé€‰æ‹©
            if candidate.quality_score >= 0.8:
                should_select = True
            
            # åˆ†å—å¤§å°æ§åˆ¶
            elif current_chunk_size + chunk_size >= self.max_chunk_size:
                should_select = True  # å¼ºåˆ¶åˆ†å‰²ï¼Œé¿å…è¶…å¤§åˆ†å—
            
            elif current_chunk_size + chunk_size >= self.target_chunk_size:
                # åœ¨ç›®æ ‡å¤§å°é™„è¿‘ï¼Œæ ¹æ®è¾¹ç•Œè´¨é‡å†³å®š
                if candidate.quality_score >= 0.6:
                    should_select = True
            
            # æ•°å­—æ ‡é¢˜ç‰¹æ®Šå¤„ç†
            elif candidate.boundary_type == BoundaryType.NUMERIC_TITLE and candidate.quality_score >= 0.7:
                should_select = True
            
            if should_select:
                selected_boundaries.append(candidate.line_index)
                current_pos = candidate.line_index
                current_chunk_size = 0
            else:
                current_chunk_size += chunk_size
        
        # å¼ºåˆ¶æ·»åŠ æ–‡æ¡£ç»“æŸ
        doc_end = next((c for c in candidates if c.boundary_type == BoundaryType.DOCUMENT_END), None)
        if doc_end and doc_end.line_index not in selected_boundaries:
            selected_boundaries.append(doc_end.line_index)
        
        # å»é‡å¹¶æ’åº
        selected_boundaries = sorted(list(set(selected_boundaries)))
        
        print(f"âœ… é€‰æ‹©äº† {len(selected_boundaries)} ä¸ªæœ€ä¼˜è¾¹ç•Œç‚¹")
        return selected_boundaries
    
    def create_chunks(self, lines: List[str], boundaries: List[int]) -> List[Dict[str, any]]:
        """
        æ ¹æ®è¾¹ç•Œåˆ›å»ºåˆ†å—ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
        
        Args:
            lines (List[str]): æ–‡æ¡£è¡Œåˆ—è¡¨
            boundaries (List[int]): è¾¹ç•Œè¡Œç´¢å¼•åˆ—è¡¨
            
        Returns:
            List[Dict[str, any]]: åˆ†å—åˆ—è¡¨
        """
        if not boundaries:
            return []
        
        chunks = []
        chunk_id = 1
        
        for i in range(len(boundaries) - 1):
            start_line = boundaries[i]
            end_line = boundaries[i + 1]
            
            # æå–åˆ†å—å†…å®¹
            chunk_lines = lines[start_line:end_line]
            chunk_content = '\n'.join(chunk_lines)
            
            # æ¸…ç†ç©ºç™½å†…å®¹
            chunk_content = chunk_content.strip()
            if not chunk_content:
                continue
            
            # æ£€æµ‹åˆ†å—ç±»å‹
            chunk_type = self.detect_chunk_type(chunk_lines)
            
            # è®¡ç®—åˆ†å—ç»Ÿè®¡ä¿¡æ¯
            char_count = len(chunk_content)
            line_count = len([line for line in chunk_lines if line.strip()])
            
            # åˆ†å—è´¨é‡è¯„ä¼°
            quality_score = self.evaluate_chunk_quality(chunk_content, chunk_type)
            
            chunk_info = {
                'id': chunk_id,
                'start_line': start_line + 1,  # 1-based indexing
                'end_line': end_line,
                'content': chunk_content,
                'type': chunk_type,
                'char_count': char_count,
                'line_count': line_count,
                'quality_score': quality_score
            }
            
            chunks.append(chunk_info)
            chunk_id += 1
        
        print(f"ğŸ“¦ åˆ›å»ºäº† {len(chunks)} ä¸ªåˆ†å—")
        return chunks
    
    def detect_chunk_type(self, chunk_lines: List[str]) -> str:
        """
        æ£€æµ‹åˆ†å—ç±»å‹
        
        Args:
            chunk_lines (List[str]): åˆ†å—è¡Œåˆ—è¡¨
            
        Returns:
            str: åˆ†å—ç±»å‹
        """
        content = '\n'.join(chunk_lines).strip()
        
        # æ£€æµ‹å„ç§ç±»å‹
        if '=====' in content:
            return 'document_separator'
        elif re.search(r'^<\d+>', content, re.MULTILINE):
            return 'numeric_title_section'
        elif content.startswith('#'):
            return 'markdown_heading'
        elif '|' in content and content.count('|') > 4:
            return 'table_content'
        elif re.search(r'^\[\d+\]', content, re.MULTILINE):
            return 'reference_section'
        elif any(keyword in content for keyword in ['å›¾', 'è¡¨', 'æ•°æ®']):
            return 'figure_table'
        else:
            return 'text_content'
    
    def evaluate_chunk_quality(self, content: str, chunk_type: str) -> float:
        """
        è¯„ä¼°åˆ†å—è´¨é‡
        
        Args:
            content (str): åˆ†å—å†…å®¹
            chunk_type (str): åˆ†å—ç±»å‹
            
        Returns:
            float: è´¨é‡åˆ†æ•° (0.0-1.0)
        """
        quality_score = 0.5  # åŸºç¡€åˆ†æ•°
        
        # é•¿åº¦åˆç†æ€§
        char_count = len(content)
        if self.min_chunk_size <= char_count <= self.max_chunk_size:
            quality_score += 0.2
        elif char_count > self.max_chunk_size:
            quality_score -= 0.1
        
        # å†…å®¹å®Œæ•´æ€§
        if not content.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?')):
            if chunk_type in ['text_content', 'numeric_title_section']:
                quality_score -= 0.1
        
        # ç±»å‹ç‰¹å®šè¯„ä¼°
        if chunk_type == 'numeric_title_section':
            quality_score += 0.1
        elif chunk_type == 'table_content':
            quality_score += 0.05
        elif chunk_type == 'reference_section':
            quality_score += 0.05
        
        return min(max(quality_score, 0.0), 1.0)
    
    def generate_statistics(self, chunks: List[Dict[str, any]], boundaries: List[int]) -> Dict[str, any]:
        """
        ç”Ÿæˆå¤„ç†ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            chunks (List[Dict[str, any]]): åˆ†å—åˆ—è¡¨
            boundaries (List[int]): è¾¹ç•Œåˆ—è¡¨
            
        Returns:
            Dict[str, any]: ç»Ÿè®¡ä¿¡æ¯
        """
        if not chunks:
            return {}
        
        # åŸºç¡€ç»Ÿè®¡
        total_chunks = len(chunks)
        total_chars = sum(chunk['char_count'] for chunk in chunks)
        avg_chunk_size = total_chars / total_chunks if total_chunks > 0 else 0
        
        # åˆ†å—å¤§å°åˆ†å¸ƒ
        size_distribution = {
            'small': len([c for c in chunks if c['char_count'] < 500]),
            'medium': len([c for c in chunks if 500 <= c['char_count'] <= 1000]),
            'large': len([c for c in chunks if 1000 < c['char_count'] <= 2000]),
            'extra_large': len([c for c in chunks if c['char_count'] > 2000])
        }
        
        # ç±»å‹åˆ†å¸ƒ
        type_distribution = {}
        for chunk in chunks:
            chunk_type = chunk['type']
            type_distribution[chunk_type] = type_distribution.get(chunk_type, 0) + 1
        
        # è´¨é‡ç»Ÿè®¡
        quality_scores = [chunk['quality_score'] for chunk in chunks]
        avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0
        
        # è¾¹ç•Œè´¨é‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
        boundary_quality = min(avg_quality + 0.2, 1.0)  # åŸºäºåˆ†å—è´¨é‡ä¼°ç®—
        
        return {
            'total_chunks': total_chunks,
            'total_characters': total_chars,
            'average_chunk_size': round(avg_chunk_size),
            'size_distribution': size_distribution,
            'type_distribution': type_distribution,
            'average_quality_score': round(avg_quality, 2),
            'boundary_quality_score': round(boundary_quality, 2),
            'total_boundaries': len(boundaries)
        }
    
    def save_processed_document(self, chunks: List[Dict[str, any]], output_path: str):
        """
        ä¿å­˜å¤„ç†åçš„æ–‡æ¡£
        
        Args:
            chunks (List[Dict[str, any]]): åˆ†å—åˆ—è¡¨
            output_path (str): è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write("====== æ–‡æ¡£å¼€å§‹ ======\n\n")
                
                for chunk in chunks:
                    f.write(f"====== åˆ†å— {chunk['id']} ======\n")
                    f.write(f"ç±»å‹: {chunk['type']}\n")
                    f.write(f"å¤§å°: {chunk['char_count']} å­—ç¬¦\n")
                    f.write(f"è´¨é‡: {chunk['quality_score']:.2f}\n")
                    f.write(f"è¡Œæ•°: {chunk['start_line']}-{chunk['end_line']}\n")
                    f.write("å†…å®¹:\n")
                    f.write(chunk['content'])
                    f.write("\n\n")
                
                f.write("====== æ–‡æ¡£ç»“æŸ ======\n")
            
            print(f"âœ… å¤„ç†åæ–‡æ¡£å·²ä¿å­˜è‡³: {output_path}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æ–‡æ¡£å¤±è´¥: {e}")
            raise
    
    def process_document(self, input_path: str, output_path: str) -> Dict[str, any]:
        """
        å¤„ç†æ–‡æ¡£çš„ä¸»å‡½æ•°
        
        Args:
            input_path (str): è¾“å…¥æ–‡æ¡£è·¯å¾„
            output_path (str): è¾“å‡ºæ–‡æ¡£è·¯å¾„
            
        Returns:
            Dict[str, any]: å¤„ç†ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯
        """
        print(f"ğŸš€ å¼€å§‹å¤„ç†æ–‡æ¡£: {input_path}")
        print(f"ğŸ“‹ é…ç½®: ç›®æ ‡å¤§å°={self.target_chunk_size}, æœ€å¤§å¤§å°={self.max_chunk_size}")
        
        # åŠ è½½æ–‡æ¡£
        lines = self.load_document(input_path)
        
        # æŸ¥æ‰¾è¾¹ç•Œå€™é€‰ç‚¹
        candidates = self.find_boundary_candidates(lines)
        
        # é€‰æ‹©æœ€ä¼˜è¾¹ç•Œ
        boundaries = self.select_optimal_boundaries(candidates, lines)
        
        # åˆ›å»ºåˆ†å—
        chunks = self.create_chunks(lines, boundaries)
        
        # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        statistics = self.generate_statistics(chunks, boundaries)
        
        # ä¿å­˜å¤„ç†åæ–‡æ¡£
        self.save_processed_document(chunks, output_path)
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        self.print_statistics(statistics)
        
        return {
            'input_path': input_path,
            'output_path': output_path,
            'statistics': statistics,
            'chunks': len(chunks),
            'boundaries': len(boundaries)
        }
    
    def print_statistics(self, stats: Dict[str, any]):
        """
        æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            stats (Dict[str, any]): ç»Ÿè®¡ä¿¡æ¯
        """
        print("\n" + "="*50)
        print("ğŸ“Š å¤„ç†ç»Ÿè®¡ä¿¡æ¯")
        print("="*50)
        print(f"æ€»åˆ†å—æ•°: {stats.get('total_chunks', 0)}")
        print(f"æ€»å­—ç¬¦æ•°: {stats.get('total_characters', 0):,}")
        print(f"å¹³å‡åˆ†å—å¤§å°: {stats.get('average_chunk_size', 0)} å­—ç¬¦")
        print(f"å¹³å‡è´¨é‡åˆ†æ•°: {stats.get('average_quality_score', 0)}")
        print(f"è¾¹ç•Œè´¨é‡åˆ†æ•°: {stats.get('boundary_quality_score', 0)}")
        
        print(f"\nğŸ“ åˆ†å—å¤§å°åˆ†å¸ƒ:")
        size_dist = stats.get('size_distribution', {})
        print(f"  å°åˆ†å— (<500å­—ç¬¦): {size_dist.get('small', 0)}")
        print(f"  ä¸­ç­‰åˆ†å— (500-1000å­—ç¬¦): {size_dist.get('medium', 0)}")
        print(f"  å¤§åˆ†å— (1000-2000å­—ç¬¦): {size_dist.get('large', 0)}")
        print(f"  è¶…å¤§åˆ†å— (>2000å­—ç¬¦): {size_dist.get('extra_large', 0)}")
        
        print(f"\nğŸ·ï¸ åˆ†å—ç±»å‹åˆ†å¸ƒ:")
        type_dist = stats.get('type_distribution', {})
        for chunk_type, count in type_dist.items():
            print(f"  {chunk_type}: {count}")
        
        print("="*50)

def main():
    """
    ä¸»å‡½æ•° - å¤„ç†å‘½ä»¤è¡Œå‚æ•°å¹¶æ‰§è¡Œæ–‡æ¡£å¤„ç†
    
    Returns:
        int: ç¨‹åºé€€å‡ºç ï¼Œ0è¡¨ç¤ºæˆåŠŸï¼Œ1è¡¨ç¤ºå¤±è´¥
    """
    parser = argparse.ArgumentParser(description='å¢å¼ºç‰ˆæ–‡æ¡£é¢„å¤„ç†å™¨ V2')
    parser.add_argument('input_file', help='è¾“å…¥æ–‡æ¡£è·¯å¾„')
    parser.add_argument('--output', '-o', help='è¾“å‡ºæ–‡æ¡£è·¯å¾„ï¼ˆé»˜è®¤ä¸ºè¾“å…¥æ–‡ä»¶å_enhanced_v2.mdï¼‰')
    parser.add_argument('--target-size', '-t', type=int, default=800, 
                       help='ç›®æ ‡åˆ†å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼Œé»˜è®¤800ï¼‰')
    parser.add_argument('--max-size', '-m', type=int, default=1500,
                       help='æœ€å¤§åˆ†å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼Œé»˜è®¤1500ï¼‰')
    parser.add_argument('--stats-output', '-s', help='ç»Ÿè®¡ä¿¡æ¯è¾“å‡ºæ–‡ä»¶ï¼ˆJSONæ ¼å¼ï¼‰')
    
    args = parser.parse_args()
    
    # ç¡®å®šè¾“å‡ºè·¯å¾„
    if args.output:
        output_path = args.output
    else:
        input_name = args.input_file.rsplit('.', 1)[0]
        output_path = f"{input_name}_enhanced_v2.md"
    
    try:
        # åˆ›å»ºå¤„ç†å™¨
        processor = EnhancedDocumentProcessor(
            target_chunk_size=args.target_size,
            max_chunk_size=args.max_size
        )
        
        # å¤„ç†æ–‡æ¡£
        result = processor.process_document(args.input_file, output_path)
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if args.stats_output:
            with open(args.stats_output, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜è‡³: {args.stats_output}")
        
        print(f"\nâœ… æ–‡æ¡£å¤„ç†å®Œæˆï¼")
        print(f"ğŸ“„ è¾“å…¥: {args.input_file}")
        print(f"ğŸ“„ è¾“å‡º: {output_path}")
        print(f"ğŸ“¦ åˆ†å—æ•°: {result['chunks']}")
        print(f"ğŸ¯ è¾¹ç•Œæ•°: {result['boundaries']}")
        
        return 0
        
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        return 1

if __name__ == "__main__":
    exit(main())