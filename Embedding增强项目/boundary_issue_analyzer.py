#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¾¹ç•Œé—®é¢˜åˆ†æå™¨ - æ·±å…¥åˆ†æè¾¹ç•Œè´¨é‡å·®çš„æ ¹æœ¬åŸå› 
ç”¨äºè¯†åˆ«è¾¹ç•Œæ£€æµ‹ç®—æ³•çš„è–„å¼±ç¯èŠ‚å¹¶æå‡ºé’ˆå¯¹æ€§ä¼˜åŒ–æ–¹æ¡ˆ

åŠŸèƒ½ï¼š
- åˆ†æè¾¹ç•Œè´¨é‡å·®çš„åˆ†å—æ¨¡å¼
- è¯†åˆ«é—®é¢˜åˆ†å—çš„å…±åŒç‰¹å¾
- ç»Ÿè®¡ä¸åŒç±»å‹è¾¹ç•Œçš„è´¨é‡åˆ†å¸ƒ
- ç”Ÿæˆä¼˜åŒ–å»ºè®®å’Œæ”¹è¿›æ–¹æ¡ˆ

ä½œè€…: RAGé¢„å¤„ç†ä¸“å®¶
ç‰ˆæœ¬: 1.0
"""

import json
import re
import argparse
from collections import Counter, defaultdict
from typing import List, Dict, Tuple, Set

class BoundaryIssueAnalyzer:
    """è¾¹ç•Œé—®é¢˜åˆ†æå™¨ç±»"""
    
    def __init__(self, quality_report_path: str, original_doc_path: str):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            quality_report_path (str): è´¨é‡æŠ¥å‘ŠJSONæ–‡ä»¶è·¯å¾„
            original_doc_path (str): åŸå§‹å¤„ç†åæ–‡æ¡£è·¯å¾„
        """
        self.quality_report_path = quality_report_path
        self.original_doc_path = original_doc_path
        self.quality_data = {}
        self.document_lines = []
        self.chunk_boundaries = []
        
    def load_data(self) -> bool:
        """
        åŠ è½½è´¨é‡æŠ¥å‘Šå’ŒåŸå§‹æ–‡æ¡£æ•°æ®
        
        Returns:
            bool: åŠ è½½æ˜¯å¦æˆåŠŸ
        """
        try:
            # åŠ è½½è´¨é‡æŠ¥å‘Š
            with open(self.quality_report_path, 'r', encoding='utf-8') as f:
                self.quality_data = json.load(f)
            
            # åŠ è½½åŸå§‹æ–‡æ¡£
            with open(self.original_doc_path, 'r', encoding='utf-8') as f:
                self.document_lines = f.readlines()
            
            print(f"âœ… æˆåŠŸåŠ è½½è´¨é‡æŠ¥å‘Šå’Œæ–‡æ¡£æ•°æ®")
            print(f"   - è´¨é‡æŠ¥å‘Šé—®é¢˜æ•°: {len(self.quality_data.get('issues', []))}")
            print(f"   - æ–‡æ¡£æ€»è¡Œæ•°: {len(self.document_lines)}")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return False
    
    def extract_chunk_boundaries(self) -> List[Tuple[int, str, str]]:
        """
        ä»æ–‡æ¡£ä¸­æå–åˆ†å—è¾¹ç•Œä¿¡æ¯
        
        Returns:
            List[Tuple[int, str, str]]: (è¡Œå·, è¾¹ç•Œç±»å‹, å†…å®¹) çš„åˆ—è¡¨
        """
        boundaries = []
        
        for i, line in enumerate(self.document_lines):
            line = line.strip()
            
            # æ£€æµ‹å„ç§è¾¹ç•Œç±»å‹
            if line.startswith('====== åˆ†å—'):
                boundaries.append((i, 'chunk_separator', line))
            elif re.match(r'^<\d+>', line):
                boundaries.append((i, 'numeric_title', line))
            elif line.startswith('#'):
                boundaries.append((i, 'markdown_heading', line))
            elif line.startswith('|') and line.endswith('|'):
                boundaries.append((i, 'table_row', line))
            elif re.match(r'^\[\d+\]', line):
                boundaries.append((i, 'reference', line))
            elif line == '':
                boundaries.append((i, 'empty_line', line))
        
        self.chunk_boundaries = boundaries
        return boundaries
    
    def analyze_poor_boundary_patterns(self) -> Dict[str, any]:
        """
        åˆ†æè¾¹ç•Œè´¨é‡å·®çš„æ¨¡å¼
        
        Returns:
            Dict[str, any]: åˆ†æç»“æœ
        """
        poor_boundary_issues = [
            issue for issue in self.quality_data.get('issues', [])
            if issue.get('type') == 'poor_boundary'
        ]
        
        print(f"\nğŸ” åˆ†æ {len(poor_boundary_issues)} ä¸ªè¾¹ç•Œè´¨é‡å·®é—®é¢˜")
        
        # åˆ†æé—®é¢˜åˆ†å—çš„å†…å®¹æ¨¡å¼
        content_patterns = defaultdict(int)
        length_distribution = []
        starting_patterns = defaultdict(int)
        
        for issue in poor_boundary_issues:
            preview = issue.get('chunk_preview', '')
            chunk_index = issue.get('chunk_index', 0)
            
            # ç»Ÿè®¡å†…å®¹é•¿åº¦
            length_distribution.append(len(preview))
            
            # åˆ†æèµ·å§‹æ¨¡å¼
            if preview.startswith('<'):
                if re.match(r'^<\d+>', preview):
                    starting_patterns['numeric_marker'] += 1
                else:
                    starting_patterns['other_bracket'] += 1
            elif preview.startswith('#'):
                starting_patterns['markdown_heading'] += 1
            elif preview.startswith('===='):
                starting_patterns['separator'] += 1
            elif preview.startswith('['):
                starting_patterns['reference'] += 1
            elif preview.startswith('|'):
                starting_patterns['table'] += 1
            else:
                starting_patterns['text_content'] += 1
            
            # åˆ†æå†…å®¹ç±»å‹
            if '=====' in preview:
                content_patterns['document_separator'] += 1
            elif re.search(r'<\d+>', preview):
                content_patterns['has_numeric_marker'] += 1
            elif 'ç‰ˆæƒ' in preview or 'ä¾µæƒ' in preview:
                content_patterns['copyright_info'] += 1
            elif 'ä¸“å®¶' in preview or 'å§”å‘˜ä¼š' in preview:
                content_patterns['expert_info'] += 1
            elif 'ç›®å½•' in preview:
                content_patterns['table_of_contents'] += 1
            elif re.search(r'\[\d+\]', preview):
                content_patterns['reference_content'] += 1
            elif 'Â·' in preview:
                content_patterns['bullet_point'] += 1
            else:
                content_patterns['general_text'] += 1
        
        return {
            'total_poor_boundaries': len(poor_boundary_issues),
            'content_patterns': dict(content_patterns),
            'starting_patterns': dict(starting_patterns),
            'length_stats': {
                'min': min(length_distribution) if length_distribution else 0,
                'max': max(length_distribution) if length_distribution else 0,
                'avg': sum(length_distribution) / len(length_distribution) if length_distribution else 0
            }
        }
    
    def analyze_boundary_quality_distribution(self) -> Dict[str, any]:
        """
        åˆ†æè¾¹ç•Œè´¨é‡åˆ†æ•°åˆ†å¸ƒ
        
        Returns:
            Dict[str, any]: è¾¹ç•Œè´¨é‡åˆ†æç»“æœ
        """
        boundary_analysis = self.quality_data.get('boundary_analysis', {})
        quality_scores = boundary_analysis.get('quality_scores', [])
        
        if not quality_scores:
            return {'error': 'æ— è¾¹ç•Œè´¨é‡åˆ†æ•°æ•°æ®'}
        
        # ç»Ÿè®¡è´¨é‡åˆ†æ•°åˆ†å¸ƒ
        score_distribution = Counter()
        for score in quality_scores:
            score_range = f"{score:.1f}"
            score_distribution[score_range] += 1
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        good_boundaries = sum(1 for score in quality_scores if score >= 0.7)
        poor_boundaries = sum(1 for score in quality_scores if score <= 0.3)
        
        return {
            'total_boundaries': len(quality_scores),
            'good_boundaries': good_boundaries,
            'poor_boundaries': poor_boundaries,
            'average_quality': sum(quality_scores) / len(quality_scores),
            'score_distribution': dict(score_distribution.most_common()),
            'quality_percentage': {
                'good': (good_boundaries / len(quality_scores)) * 100,
                'poor': (poor_boundaries / len(quality_scores)) * 100
            }
        }
    
    def identify_problematic_sections(self) -> List[Dict[str, any]]:
        """
        è¯†åˆ«é—®é¢˜é›†ä¸­çš„æ–‡æ¡£åŒºåŸŸ
        
        Returns:
            List[Dict[str, any]]: é—®é¢˜åŒºåŸŸåˆ—è¡¨
        """
        poor_boundary_issues = [
            issue for issue in self.quality_data.get('issues', [])
            if issue.get('type') == 'poor_boundary'
        ]
        
        # æŒ‰åˆ†å—ç´¢å¼•åˆ†ç»„
        chunk_issue_counts = Counter()
        for issue in poor_boundary_issues:
            chunk_index = issue.get('chunk_index', 0)
            chunk_issue_counts[chunk_index] += 1
        
        # è¯†åˆ«é—®é¢˜é›†ä¸­åŒºåŸŸï¼ˆè¿ç»­çš„é—®é¢˜åˆ†å—ï¼‰
        problematic_sections = []
        current_section = None
        
        for chunk_index in sorted(chunk_issue_counts.keys()):
            if current_section is None:
                current_section = {
                    'start_chunk': chunk_index,
                    'end_chunk': chunk_index,
                    'issue_count': chunk_issue_counts[chunk_index],
                    'chunk_previews': []
                }
            elif chunk_index == current_section['end_chunk'] + 1:
                # è¿ç»­åŒºåŸŸ
                current_section['end_chunk'] = chunk_index
                current_section['issue_count'] += chunk_issue_counts[chunk_index]
            else:
                # æ–°åŒºåŸŸå¼€å§‹
                if current_section['issue_count'] >= 3:  # åªè®°å½•é—®é¢˜è¾ƒå¤šçš„åŒºåŸŸ
                    problematic_sections.append(current_section)
                
                current_section = {
                    'start_chunk': chunk_index,
                    'end_chunk': chunk_index,
                    'issue_count': chunk_issue_counts[chunk_index],
                    'chunk_previews': []
                }
        
        # æ·»åŠ æœ€åä¸€ä¸ªåŒºåŸŸ
        if current_section and current_section['issue_count'] >= 3:
            problematic_sections.append(current_section)
        
        # ä¸ºæ¯ä¸ªåŒºåŸŸæ·»åŠ é¢„è§ˆä¿¡æ¯
        for section in problematic_sections:
            for issue in poor_boundary_issues:
                chunk_index = issue.get('chunk_index', 0)
                if section['start_chunk'] <= chunk_index <= section['end_chunk']:
                    section['chunk_previews'].append({
                        'chunk_index': chunk_index,
                        'preview': issue.get('chunk_preview', '')[:50]
                    })
        
        return problematic_sections
    
    def generate_optimization_recommendations(self, analysis_results: Dict[str, any]) -> List[str]:
        """
        åŸºäºåˆ†æç»“æœç”Ÿæˆä¼˜åŒ–å»ºè®®
        
        Args:
            analysis_results (Dict[str, any]): åˆ†æç»“æœ
            
        Returns:
            List[str]: ä¼˜åŒ–å»ºè®®åˆ—è¡¨
        """
        recommendations = []
        
        pattern_analysis = analysis_results.get('pattern_analysis', {})
        quality_analysis = analysis_results.get('quality_analysis', {})
        
        # åŸºäºå†…å®¹æ¨¡å¼çš„å»ºè®®
        content_patterns = pattern_analysis.get('content_patterns', {})
        if content_patterns.get('has_numeric_marker', 0) > 20:
            recommendations.append(
                "ğŸ”§ ä¼˜åŒ–æ•°å­—æ ‡è®°è¾¹ç•Œæ£€æµ‹ï¼šå¤§é‡åˆ†å—åŒ…å«<æ•°å­—>æ ‡è®°ä½†è¾¹ç•Œè´¨é‡å·®ï¼Œ"
                "å»ºè®®æ”¹è¿›æ•°å­—æ ‡è®°çš„è¾¹ç•Œä¼˜å…ˆçº§ç®—æ³•"
            )
        
        if content_patterns.get('reference_content', 0) > 30:
            recommendations.append(
                "ğŸ“š æ”¹è¿›å¼•ç”¨æ–‡çŒ®å¤„ç†ï¼šå¼•ç”¨å†…å®¹åˆ†å—è¾¹ç•Œè´¨é‡æ™®éè¾ƒå·®ï¼Œ"
                "å»ºè®®ä¸“é—¨ä¼˜åŒ–å¼•ç”¨æ–‡çŒ®çš„åˆ†å—é€»è¾‘"
            )
        
        if content_patterns.get('document_separator', 0) > 5:
            recommendations.append(
                "ğŸ“„ ä¼˜åŒ–æ–‡æ¡£åˆ†éš”ç¬¦å¤„ç†ï¼šæ–‡æ¡£åˆ†éš”ç¬¦åº”è¯¥æœ‰æ›´é«˜çš„è¾¹ç•Œä¼˜å…ˆçº§ï¼Œ"
                "é¿å…è¢«è¯¯åˆ¤ä¸ºä½è´¨é‡è¾¹ç•Œ"
            )
        
        # åŸºäºè´¨é‡åˆ†å¸ƒçš„å»ºè®®
        poor_percentage = quality_analysis.get('quality_percentage', {}).get('poor', 0)
        if poor_percentage > 30:
            recommendations.append(
                f"âš ï¸ æ•´ä½“è¾¹ç•Œè´¨é‡éœ€è¦æå‡ï¼š{poor_percentage:.1f}%çš„è¾¹ç•Œè´¨é‡è¾ƒå·®ï¼Œ"
                "å»ºè®®é‡æ–°è®¾è®¡è¾¹ç•Œè¯„åˆ†ç®—æ³•"
            )
        
        # åŸºäºèµ·å§‹æ¨¡å¼çš„å»ºè®®
        starting_patterns = pattern_analysis.get('starting_patterns', {})
        if starting_patterns.get('text_content', 0) > starting_patterns.get('numeric_marker', 0):
            recommendations.append(
                "ğŸ¯ åŠ å¼ºè¯­ä¹‰è¾¹ç•Œæ£€æµ‹ï¼šå¤§é‡é—®é¢˜åˆ†å—ä»¥æ™®é€šæ–‡æœ¬å¼€å§‹ï¼Œ"
                "å»ºè®®å¢åŠ åŸºäºè¯­ä¹‰çš„è¾¹ç•Œæ£€æµ‹é€»è¾‘"
            )
        
        return recommendations
    
    def run_analysis(self) -> Dict[str, any]:
        """
        è¿è¡Œå®Œæ•´çš„è¾¹ç•Œé—®é¢˜åˆ†æ
        
        Returns:
            Dict[str, any]: å®Œæ•´åˆ†æç»“æœ
        """
        if not self.load_data():
            return {'error': 'æ•°æ®åŠ è½½å¤±è´¥'}
        
        print("\nğŸ” å¼€å§‹è¾¹ç•Œé—®é¢˜æ·±åº¦åˆ†æ...")
        
        # æå–è¾¹ç•Œä¿¡æ¯
        boundaries = self.extract_chunk_boundaries()
        
        # åˆ†æè¾¹ç•Œè´¨é‡å·®çš„æ¨¡å¼
        pattern_analysis = self.analyze_poor_boundary_patterns()
        
        # åˆ†æè¾¹ç•Œè´¨é‡åˆ†å¸ƒ
        quality_analysis = self.analyze_boundary_quality_distribution()
        
        # è¯†åˆ«é—®é¢˜åŒºåŸŸ
        problematic_sections = self.identify_problematic_sections()
        
        # ç”Ÿæˆåˆ†æç»“æœ
        analysis_results = {
            'pattern_analysis': pattern_analysis,
            'quality_analysis': quality_analysis,
            'problematic_sections': problematic_sections,
            'boundary_count': len(boundaries)
        }
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        recommendations = self.generate_optimization_recommendations(analysis_results)
        analysis_results['recommendations'] = recommendations
        
        return analysis_results
    
    def print_analysis_report(self, results: Dict[str, any]):
        """
        æ‰“å°åˆ†ææŠ¥å‘Š
        
        Args:
            results (Dict[str, any]): åˆ†æç»“æœ
        """
        print("\n" + "="*60)
        print("ğŸ“Š è¾¹ç•Œé—®é¢˜æ·±åº¦åˆ†ææŠ¥å‘Š")
        print("="*60)
        
        # æ¨¡å¼åˆ†æ
        pattern_analysis = results.get('pattern_analysis', {})
        print(f"\nğŸ” é—®é¢˜æ¨¡å¼åˆ†æ:")
        print(f"   æ€»é—®é¢˜æ•°: {pattern_analysis.get('total_poor_boundaries', 0)}")
        
        print(f"\nğŸ“‹ å†…å®¹ç±»å‹åˆ†å¸ƒ:")
        for content_type, count in pattern_analysis.get('content_patterns', {}).items():
            print(f"   â€¢ {content_type}: {count} ä¸ª")
        
        print(f"\nğŸ¯ èµ·å§‹æ¨¡å¼åˆ†å¸ƒ:")
        for pattern, count in pattern_analysis.get('starting_patterns', {}).items():
            print(f"   â€¢ {pattern}: {count} ä¸ª")
        
        # è´¨é‡åˆ†æ
        quality_analysis = results.get('quality_analysis', {})
        if 'error' not in quality_analysis:
            print(f"\nğŸ“ˆ è¾¹ç•Œè´¨é‡åˆ†å¸ƒ:")
            print(f"   æ€»è¾¹ç•Œæ•°: {quality_analysis.get('total_boundaries', 0)}")
            print(f"   ä¼˜è´¨è¾¹ç•Œ: {quality_analysis.get('good_boundaries', 0)} ä¸ª "
                  f"({quality_analysis.get('quality_percentage', {}).get('good', 0):.1f}%)")
            print(f"   ä½è´¨è¾¹ç•Œ: {quality_analysis.get('poor_boundaries', 0)} ä¸ª "
                  f"({quality_analysis.get('quality_percentage', {}).get('poor', 0):.1f}%)")
            print(f"   å¹³å‡è´¨é‡: {quality_analysis.get('average_quality', 0):.3f}")
        
        # é—®é¢˜åŒºåŸŸ
        problematic_sections = results.get('problematic_sections', [])
        if problematic_sections:
            print(f"\nğŸ¯ é—®é¢˜é›†ä¸­åŒºåŸŸ ({len(problematic_sections)} ä¸ª):")
            for i, section in enumerate(problematic_sections, 1):
                print(f"   {i}. åˆ†å— {section['start_chunk']+1}-{section['end_chunk']+1}: "
                      f"{section['issue_count']} ä¸ªé—®é¢˜")
        
        # ä¼˜åŒ–å»ºè®®
        recommendations = results.get('recommendations', [])
        if recommendations:
            print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
            for i, rec in enumerate(recommendations, 1):
                print(f"   {i}. {rec}")
        
        print("\n" + "="*60)

def main():
    """
    ä¸»å‡½æ•° - å¤„ç†å‘½ä»¤è¡Œå‚æ•°å¹¶æ‰§è¡Œè¾¹ç•Œé—®é¢˜åˆ†æ
    
    Returns:
        int: ç¨‹åºé€€å‡ºç ï¼Œ0è¡¨ç¤ºæˆåŠŸï¼Œ1è¡¨ç¤ºå¤±è´¥
    """
    parser = argparse.ArgumentParser(description='è¾¹ç•Œé—®é¢˜æ·±åº¦åˆ†æå™¨')
    parser.add_argument('--report', '-r', default='quality_report.json',
                       help='è´¨é‡æŠ¥å‘ŠJSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--document', '-d', 
                       default='To_be_processed/ä¹³è…ºç™Œè¯Šç–—æŒ‡å—2025_enhanced.md',
                       help='å¤„ç†åçš„æ–‡æ¡£è·¯å¾„')
    parser.add_argument('--output', '-o', help='è¾“å‡ºåˆ†æç»“æœåˆ°JSONæ–‡ä»¶')
    
    args = parser.parse_args()
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = BoundaryIssueAnalyzer(args.report, args.document)
    
    # è¿è¡Œåˆ†æ
    results = analyzer.run_analysis()
    
    if 'error' in results:
        print(f"âŒ åˆ†æå¤±è´¥: {results['error']}")
        return 1
    
    # æ‰“å°æŠ¥å‘Š
    analyzer.print_analysis_report(results)
    
    # ä¿å­˜ç»“æœï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if args.output:
        try:
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"\nâœ… åˆ†æç»“æœå·²ä¿å­˜è‡³: {args.output}")
        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
    
    return 0

if __name__ == "__main__":
    exit(main())